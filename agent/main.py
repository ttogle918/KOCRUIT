from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
import sys
import os
from datetime import datetime

from .agents.graph_agent import build_graph
from .agents.chatbot_graph import create_chatbot_graph, initialize_chat_state, create_session_id
from .agents.chatbot_node import ChatbotNode
from .redis_monitor import RedisMonitor
from .scheduler import RedisScheduler
from .api.v2.analysis import router as analysis_router  # ë¶„ì„ ê´€ë ¨ API ë¼ìš°í„° ì¶”ê°€
from tools.weight_extraction_tool import weight_extraction_tool
from tools.form_fill_tool import form_fill_tool, form_improve_tool
from tools.form_edit_tool import form_edit_tool, form_status_check_tool
from tools.form_improve_tool import form_improve_tool
from .agents.application_evaluation_agent import evaluate_application
from tools.speech_recognition_tool import speech_recognition_tool
from tools.highlight_tool import highlight_resume_content
# from tools.realtime_interview_evaluation_tool import realtime_interview_evaluation_tool, RealtimeInterviewEvaluationTool
from dotenv import load_dotenv
import uuid
import os
from fastapi import HTTPException
from langchain_openai import ChatOpenAI
import json
from pydantic import BaseModel
from typing import Optional

from fastapi import FastAPI, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
import tempfile
import os
from tools.speech_recognition_tool import SpeechRecognitionTool
from tools.realtime_interview_evaluation_tool import RealtimeInterviewEvaluationTool
from tools.answer_grading_tool import grade_written_test_answer

# í™”ì ë¶„ë¦¬ ë° ë¹„ë””ì˜¤ ìë¥´ê¸° ê´€ë ¨
import base64
import tempfile
import subprocess
import whisper
import librosa
import numpy as np
from typing import List, Dict, Any, Optional
import soundfile as sf
from pyannote.audio import Pipeline
from pyannote.audio.pipelines.utils.hook import ProgressHook

# Python ê²½ë¡œì— í˜„ì¬ ë””ë ‰í† ë¦¬ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

load_dotenv()

app = FastAPI(
    title="AI Agent API",
    description="AI Agent for KOCruit Project",
    version="1.0.0"
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # í”„ë¡ íŠ¸ì—”ë“œ ì£¼ì†Œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic ëª¨ë¸ ì •ì˜
class HighlightResumeRequest(BaseModel):
    text: str
    job_description: str = ""
    company_values: str = ""
    jobpost_id: Optional[int] = None
    company_id: Optional[int] = None

class SpeakerAnalysisRequest(BaseModel):
    audio_data: str  # base64 encoded audio
    video_data: str  # base64 encoded video
    audio_filename: str
    video_filename: str

class WhisperAnalysisRequest(BaseModel):
    audio_path: str
    application_id: Optional[int] = None

class QAAnalysisRequest(BaseModel):
    audio_path: str
    application_id: Optional[int] = None
    persist: Optional[bool] = False
    output_dir: Optional[str] = None
    max_workers: Optional[int] = 2
    delete_after_input: Optional[bool] = False
    run_emotion_context: Optional[bool] = False

# API ë¼ìš°í„° ë“±ë¡
app.include_router(analysis_router, prefix="/api/v2/agent", tags=["Analysis"])

# í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy", "message": "Kocruit Agent API is running"}

@app.get("/")
async def root():
    """ë£¨íŠ¸ ê²½ë¡œ - API ì •ë³´ ë°˜í™˜"""
    return {
        "message": "AI Agent API is running",
        "version": "1.0.0",
        "endpoints": {
            "chat": "/chat/",
            "highlight_resume": "/highlight-resume",
            "extract_weights": "/extract-weights/",
            "evaluate_application": "/evaluate-application/",
            "monitor_health": "/monitor/health",
            "monitor_sessions": "/monitor/sessions",
            "speech_recognition": "/agent/speech-recognition",
            "realtime_evaluation": "/agent/realtime-interview-evaluation",
            "speaker_analysis_and_trim": "/speaker-analysis-and-trim",
            "docs": "/docs"
        }
    }

# í™”ì ë¶„ë¦¬ ë° ë¹„ë””ì˜¤ ìë¥´ê¸° í´ë˜ìŠ¤
class SpeakerAnalysisService:
    def __init__(self):
        self.whisper_model = None
        self.speaker_pipeline = None
        self._initialize_models()
    
    def _initialize_models(self):
        """AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        try:
            print("í™”ì ë¶„ë¦¬ ì„œë¹„ìŠ¤ ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # Whisper ëª¨ë¸ ë¡œë“œ (ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©)
            self.whisper_model = whisper.load_model("tiny")  # base â†’ tinyë¡œ ë³€ê²½
            print("Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (tiny ëª¨ë¸)")
            
            # í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (HuggingFace í† í° ì§€ì›)
            try:
                auth_token = os.environ.get("HUGGINGFACE_TOKEN") or os.environ.get("HF_TOKEN")
                if auth_token:
                    self.speaker_pipeline = Pipeline.from_pretrained(
                        "pyannote/speaker-diarization-3.1",
                        use_auth_token=auth_token
                    )
                else:
                    self.speaker_pipeline = Pipeline.from_pretrained(
                        "pyannote/speaker-diarization-3.1"
                    )
                print("í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                self.speaker_pipeline = None
            
        except Exception as e:
            print(f"ëª¨ë¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
            raise
    
    def extract_applicant_audio(self, audio_path: str) -> List[Dict[str, float]]:
        """í™”ì ë¶„ë¦¬ë¥¼ í†µí•´ ë©´ì ‘ì ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            if not self.speaker_pipeline:
                print("í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ì´ ì—†ì–´ ê¸°ë³¸ ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
                return []
            
            print("í™”ì ë¶„ë¦¬ ì‹œì‘...")
            
            # í™”ì ë¶„ë¦¬ ì‹¤í–‰
            diarization = self.speaker_pipeline(audio_path)
            
            # í™”ìë³„ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
            speaker_segments = {}
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                if speaker not in speaker_segments:
                    speaker_segments[speaker] = []
                speaker_segments[speaker].append({
                    'start': turn.start,
                    'end': turn.end,
                    'duration': turn.end - turn.start
                })
            
            # ë©´ì ‘ì ì‹ë³„ (ê°€ì¥ ê¸´ ë°œí™” ì‹œê°„ì„ ê°€ì§„ í™”ì)
            if not speaker_segments:
                return []
            
            applicant_speaker = max(speaker_segments.keys(), 
                                  key=lambda s: sum(seg['duration'] for seg in speaker_segments[s]))
            
            print(f"ë©´ì ‘ì í™”ì ì‹ë³„: {applicant_speaker}")
            print(f"ë©´ì ‘ì ë°œí™” ì„¸ê·¸ë¨¼íŠ¸: {len(speaker_segments[applicant_speaker])}ê°œ")
            
            return speaker_segments[applicant_speaker]
            
        except Exception as e:
            print(f"í™”ì ë¶„ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def trim_video_by_applicant_speech(self, video_path: str, applicant_segments: List[Dict], max_duration: int = 30) -> Optional[str]:
        """ë©´ì ‘ì ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ìƒì„ ìë¦…ë‹ˆë‹¤."""
        try:
            if not applicant_segments:
                print("ë©´ì ‘ì ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ì–´ ìë¥´ê¸°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
                return video_path
            
            # 30ì´ˆ ì´ë‚´ì˜ ë©´ì ‘ì ë°œí™” ì„¸ê·¸ë¨¼íŠ¸ë“¤ ì°¾ê¸°
            valid_segments = []
            current_duration = 0
            
            for segment in applicant_segments:
                segment_end = segment.get("end", 0)
                if segment_end <= max_duration:
                    valid_segments.append(segment)
                    current_duration = segment_end
                else:
                    break
            
            if not valid_segments:
                print("ìœ íš¨í•œ ë©´ì ‘ì ë°œí™” ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                return video_path
            
            # ìë¥¼ ì‹œê°„ ê³„ì‚° (ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ëê¹Œì§€)
            trim_duration = current_duration
            
            # ì´ë¯¸ 30ì´ˆ ì´í•˜ë©´ ìë¥´ì§€ ì•ŠìŒ
            if trim_duration >= max_duration:
                print(f"ë©´ì ‘ì ë°œí™”ê°€ ì´ë¯¸ {trim_duration:.1f}ì´ˆë¡œ ì ì ˆí•œ ê¸¸ì´ì…ë‹ˆë‹¤")
                return video_path
            
            # ìë¥¸ ì˜ìƒ íŒŒì¼ ìƒì„±
            trimmed_path = tempfile.mktemp(suffix=".mp4")
            
            cmd = [
                "ffmpeg", "-i", video_path,
                "-t", str(trim_duration),
                "-c", "copy",  # ì¬ì¸ì½”ë”© ì—†ì´ ë¹ ë¥¸ ìë¥´ê¸°
                "-y", trimmed_path
            ]
            
            print(f"ë©´ì ‘ì ìŒì„± ê¸°ë°˜ ì˜ìƒ ìë¥´ê¸°: {trim_duration:.1f}ì´ˆ")
            subprocess.run(cmd, check=True, capture_output=True)
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            original_size = os.path.getsize(video_path) / (1024 * 1024)  # MB
            trimmed_size = os.path.getsize(trimmed_path) / (1024 * 1024)  # MB
            
            print(f"ì˜ìƒ í¬ê¸°: {original_size:.1f}MB â†’ {trimmed_size:.1f}MB (ì ˆì•½: {original_size - trimmed_size:.1f}MB)")
            
            return trimmed_path
            
        except Exception as e:
            print(f"ë©´ì ‘ì ìŒì„± ê¸°ë°˜ ì˜ìƒ ìë¥´ê¸° ì˜¤ë¥˜: {str(e)}")
            return video_path
    
    def create_video_segments_by_questions(self, video_path: str, audio_path: str, max_segment_duration: int = 60) -> List[Dict]:
        """
        ì§ˆë¬¸ë³„ë¡œ ë¹„ë””ì˜¤ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            video_path: ì›ë³¸ ë¹„ë””ì˜¤ ê²½ë¡œ
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            max_segment_duration: ìµœëŒ€ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
            
        Returns:
            ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            print("ì§ˆë¬¸ë³„ ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ ì‹œì‘...")
            
            # Whisperë¡œ ìŒì„± ì¸ì‹ ë° íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
            result = self.whisper_model.transcribe(audio_path, word_timestamps=True)
            
            # ì§ˆë¬¸ í‚¤ì›Œë“œ ê°ì§€ (ë©´ì ‘ê´€ ì§ˆë¬¸ íŒ¨í„´)
            question_keywords = [
                "ì§ˆë¬¸", "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””ì„œ", "ë¬´ì—‡ì„", "ì–´ë–¤", "ì„¤ëª…í•´ì£¼ì„¸ìš”", 
                "ì´ìœ ëŠ”", "ê²½í—˜", "ê³„íš", "ëª©í‘œ", "ì¥ì ", "ë‹¨ì ", "í•´ê²°", "ë„ì „", "ì„±ê³µ", "ì‹¤íŒ¨"
            ]
            
            segments = []
            current_start = 0
            current_segment_duration = 0
            
            for segment in result.get("segments", []):
                text = segment.get("text", "").lower()
                start_time = segment.get("start", 0)
                end_time = segment.get("end", 0)
                
                # ì§ˆë¬¸ ê°ì§€ ë˜ëŠ” ìµœëŒ€ ê¸¸ì´ ë„ë‹¬
                is_question = any(keyword in text for keyword in question_keywords)
                segment_duration = end_time - start_time
                
                if is_question or (current_segment_duration + segment_duration) > max_segment_duration:
                    # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
                    if current_segment_duration > 10:  # ìµœì†Œ 10ì´ˆ ì´ìƒ
                        segment_path = self._extract_video_segment(
                            video_path, current_start, start_time, len(segments) + 1
                        )
                        if segment_path:
                            segments.append({
                                'segment_index': len(segments) + 1,
                                'start_time': current_start,
                                'end_time': start_time,
                                'duration': start_time - current_start,
                                'file_path': segment_path,
                                'text': text,
                                'is_question': is_question
                            })
                    
                    # ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘
                    current_start = start_time
                    current_segment_duration = segment_duration
                else:
                    current_segment_duration += segment_duration
            
            # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
            if current_segment_duration > 10:
                segment_path = self._extract_video_segment(
                    video_path, current_start, result.get("segments", [{}])[-1].get("end", 0), len(segments) + 1
                )
                if segment_path:
                    segments.append({
                        'segment_index': len(segments) + 1,
                        'start_time': current_start,
                        'end_time': result.get("segments", [{}])[-1].get("end", 0),
                        'duration': result.get("segments", [{}])[-1].get("end", 0) - current_start,
                        'file_path': segment_path,
                        'text': result.get("segments", [{}])[-1].get("text", ""),
                        'is_question': False
                    })
            
            print(f"ì§ˆë¬¸ë³„ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            return segments
            
        except Exception as e:
            print(f"ì§ˆë¬¸ë³„ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def _extract_video_segment(self, video_path: str, start_time: float, end_time: float, segment_index: int) -> Optional[str]:
        """ë¹„ë””ì˜¤ì—ì„œ íŠ¹ì • ì‹œê°„ êµ¬ê°„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            duration = end_time - start_time
            if duration < 5:  # 5ì´ˆ ë¯¸ë§Œì€ ê±´ë„ˆë›°ê¸°
                return None
            
            segment_path = tempfile.mktemp(suffix=f"_segment_{segment_index}.mp4")
            
            cmd = [
                "ffmpeg", "-i", video_path,
                "-ss", str(start_time),
                "-t", str(duration),
                "-c", "copy",  # ì¬ì¸ì½”ë”© ì—†ì´ ë¹ ë¥¸ ì¶”ì¶œ
                "-y", segment_path
            ]
            
            subprocess.run(cmd, check=True, capture_output=True)
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(segment_path) / (1024 * 1024)  # MB
            print(f"ì„¸ê·¸ë¨¼íŠ¸ {segment_index} ì¶”ì¶œ ì™„ë£Œ: {duration:.1f}ì´ˆ, {file_size:.1f}MB")
            
            return segment_path
            
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _get_video_duration(self, video_path: str) -> Optional[float]:
        """ë¹„ë””ì˜¤ ê¸¸ì´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            cmd = [
                "ffprobe", "-v", "quiet", "-show_entries", "format=duration",
                "-of", "csv=p=0", video_path
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            return float(result.stdout.strip())
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ê¸¸ì´ í™•ì¸ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def analyze_audio_with_whisper(self, audio_path: str) -> Dict[str, Any]:
        """Whisperë¡œ ìŒì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            print(f"ğŸ¤ Whisper ë¶„ì„ ì‹œì‘: {audio_path}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(audio_path):
                print(f"âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {audio_path}")
                return {
                    "text": "",
                    "transcription": "",
                    "speech_rate": 0,
                    "segments_count": 0,
                    "duration": 0,
                    "language": "ko"
                }
            
            # Whisperë¡œ ìŒì„± ì¸ì‹
            result = self.whisper_model.transcribe(audio_path, word_timestamps=True)
            transcription = result.get("text", "")
            segments = result.get("segments", [])
            language = result.get("language", "ko")
            
            print(f"âœ… Whisper ë¶„ì„ ì™„ë£Œ: {len(transcription)} ë¬¸ì")
            
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            audio, sr = librosa.load(audio_path, sr=16000)
            
            # ë°œí™” ì†ë„ ê³„ì‚°
            speech_rate = self._calculate_speech_rate(transcription, len(audio) / sr)
            
            return {
                "text": transcription,  # 'text' í‚¤ ì¶”ê°€
                "transcription": transcription,
                "speech_rate": round(speech_rate, 3),
                "segments_count": len(segments),
                "duration": len(audio) / sr,
                "language": language,
                "segments": segments
            }
            
        except Exception as e:
            print(f"âŒ Whisper ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                "text": "",
                "transcription": "",
                "speech_rate": 0,
                "segments_count": 0,
                "duration": 0,
                "language": "ko",
                "segments": []
            }

    def _merge_contiguous_by_role(self, diar_segments: List[Dict[str, Any]], applicant_id: str) -> List[Dict[str, Any]]:
        """ì—°ì†ëœ ë™ì¼ í™”ì ì—­í• (ë©´ì ‘ê´€/ì§€ì›ì)ì„ í•˜ë‚˜ì˜ ë¸”ë¡ìœ¼ë¡œ ë³‘í•©"""
        if not diar_segments:
            return []
        # start ê¸°ì¤€ ì •ë ¬
        diar_segments = sorted(diar_segments, key=lambda s: s.get('start', 0.0))
        blocks: List[Dict[str, Any]] = []
        def role_of(sid: str) -> str:
            return "applicant" if str(sid) == str(applicant_id) else "interviewer"
        prev_role = role_of(diar_segments[0].get('speaker', 'unknown'))
        current = {
            'role': prev_role,
            'start': float(diar_segments[0].get('start', 0.0)),
            'end': float(diar_segments[0].get('end', 0.0))
        }
        for seg in diar_segments[1:]:
            r = role_of(seg.get('speaker', 'unknown'))
            s = float(seg.get('start', 0.0))
            e = float(seg.get('end', 0.0))
            if r == prev_role and s <= current['end'] + 0.2:  # ì‘ì€ ê²¹ì¹¨/ê°„ê·¹ í—ˆìš©
                current['end'] = max(current['end'], e)
            else:
                blocks.append(current)
                current = {'role': r, 'start': s, 'end': e}
                prev_role = r
        blocks.append(current)
        return blocks

    def _slice_audio(self, audio_path: str, start: float, end: float) -> Optional[str]:
        """ì˜¤ë””ì˜¤ì—ì„œ íŠ¹ì • êµ¬ê°„ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ê²½ë¡œ ë°˜í™˜"""
        try:
            if end - start <= 0.5:
                return None
            audio, sr = librosa.load(audio_path, sr=16000)
            s_idx = int(max(0.0, start) * sr)
            e_idx = int(min(len(audio) / sr, end) * sr)
            if e_idx <= s_idx:
                return None
            chunk = audio[s_idx:e_idx]
            out_path = tempfile.mktemp(suffix=f"_{int(start)}-{int(end)}s.wav")
            sf.write(out_path, chunk, sr)
            return out_path
        except Exception as e:
            print(f"ì˜¤ë””ì˜¤ ìŠ¬ë¼ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
            return None

    def build_qa_pairs_and_analyze_answers(self, audio_path: str, persist: bool = False, output_dir: Optional[str] = None, application_id: Optional[str] = None, max_workers: int = 2) -> Dict[str, Any]:
        """í™”ìë¶„ë¦¬ë¡œ Qâ†’A í˜ì–´ë¥¼ ë§Œë“¤ê³ , ì§€ì›ì ë‹µë³€ë³„ Whisper ë¶„ì„ ìˆ˜í–‰

        max_workers: ë‹µë³€ êµ¬ê°„ ë³‘ë ¬ ì „ì‚¬ ì›Œì»¤ ìˆ˜(ê¶Œì¥ 2~4)
        """
        try:
            diar_segments: List[Dict[str, Any]] = []
            if self.speaker_pipeline:
                diarization = self.speaker_pipeline(audio_path)
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    diar_segments.append({
                        'start': float(turn.start),
                        'end': float(turn.end),
                        'speaker': str(speaker),
                        'duration': float(turn.end - turn.start)
                    })
            else:
                # Fallback: pyannote ë¯¸ì´ˆê¸°í™” ì‹œ ê°„ë‹¨ í™”ì ê°ì§€ ì‚¬ìš©
                print("í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ ì—†ìŒ â†’ fallback í™”ì ê°ì§€ ì‹œë„")
                try:
                    speech_tool = SpeechRecognitionTool()
                    diar = speech_tool.detect_speakers(audio_path)
                    diar_segments = [
                        {
                            'start': float(s.get('start', 0.0)),
                            'end': float(s.get('end', 0.0)),
                            'speaker': str(s.get('speaker', 'unknown')),
                            'duration': float(s.get('duration', 0.0))
                        }
                        for s in diar.get("speakers", [])
                    ]
                except Exception as e:
                    print(f"fallback í™”ì ê°ì§€ ì˜¤ë¥˜: {str(e)}")
                    diar_segments = []
            if not diar_segments:
                return {"success": True, "qa": []}

            # ì§€ì›ì í™”ì ì‹ë³„(ì´ ë°œí™”ì‹œê°„ ìµœëŒ€)
            totals: Dict[str, float] = {}
            for s in diar_segments:
                sid = str(s.get('speaker', 'unknown'))
                totals[sid] = totals.get(sid, 0.0) + max(0.0, float(s.get('duration', 0.0)))
            applicant_id = max(totals.items(), key=lambda kv: kv[1])[0]

            # ì—­í•  ë¸”ë¡ ë³‘í•© í›„ Qâ†’A í˜ì–´ë§
            blocks = self._merge_contiguous_by_role(diar_segments, applicant_id)
            qa_pairs: List[Dict[str, Any]] = []
            i = 0
            while i < len(blocks) - 1:
                q = blocks[i]
                a = blocks[i + 1]
                if q['role'] == 'interviewer' and a['role'] == 'applicant':
                    qa_pairs.append({
                        'question': {'start': q['start'], 'end': q['end']},
                        'answer': {'start': a['start'], 'end': a['end']}
                    })
                    i += 2
                else:
                    i += 1

            # ë³´ì •: Qâ†’A í˜ì–´ê°€ ì—†ìœ¼ë©´, ì§€ì›ì ë¸”ë¡ë§Œìœ¼ë¡œ ë‹µë³€ ë‹¨ìœ„ í˜ì–´ ìƒì„±
            if not qa_pairs:
                for b in blocks:
                    if b['role'] == 'applicant':
                        qa_pairs.append({
                            'question': None,
                            'answer': {'start': b['start'], 'end': b['end']}
                        })

            # ê° ë‹µë³€ êµ¬ê°„ì— ëŒ€í•´ Whisper ë¶„ì„ (ë³‘ë ¬)
            analyzed: List[Dict[str, Any]] = []
            from concurrent.futures import ThreadPoolExecutor, as_completed

            def process_one(idx: int, pair: Dict[str, Any]) -> Dict[str, Any]:
                answer = pair['answer']
                ans_path = self._slice_audio(audio_path, answer['start'], answer['end'])
                if not ans_path:
                    return {
                        'index': idx,
                        'question': pair['question'],
                        'answer': answer,
                        'analysis': {"text": "", "transcription": "", "duration": 0, "speech_rate": 0},
                        'answer_audio_path': None
                    }
                analysis = self.analyze_audio_with_whisper(ans_path)
                saved_path = None
                if persist:
                    try:
                        base_dir = output_dir or os.path.join("/tmp", "qa_slices")
                        if application_id:
                            base_dir = os.path.join(base_dir, str(application_id))
                        os.makedirs(base_dir, exist_ok=True)
                        saved_path = os.path.join(base_dir, f"answer_{idx:02d}_{int(answer['start'])}-{int(answer['end'])}s.wav")
                        import shutil
                        shutil.move(ans_path, saved_path)
                    except Exception as e:
                        print(f"ë‹µë³€ ì˜¤ë””ì˜¤ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
                        saved_path = None
                return {
                    'index': idx,
                    'question': pair['question'],
                    'answer': answer,
                    'analysis': analysis,
                    'answer_audio_path': saved_path or ans_path
                }

            max_workers = max(1, int(max_workers or 1))
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {
                    executor.submit(process_one, idx, pair): idx
                    for idx, pair in enumerate(qa_pairs, start=1)
                }
                for future in as_completed(futures):
                    try:
                        analyzed.append(future.result())
                    except Exception as e:
                        print(f"ë³‘ë ¬ ì „ì‚¬ ì‘ì—… ì˜¤ë¥˜: {str(e)}")

            return {
                'success': True,
                'applicant_speaker_id': applicant_id,
                'qa': analyzed,
                'total_pairs': len(analyzed)
            }
        except Exception as e:
            print(f"QA ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {"success": False, "qa": [], "error": str(e)}
    
    def _calculate_speech_rate(self, transcription: str, duration: float) -> float:
        """ë§í•˜ê¸° ì†ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if not transcription or duration <= 0:
            return 0
        
        word_count = len(transcription.split())
        return word_count / duration  # ë¶„ë‹¹ ë‹¨ì–´ ìˆ˜

    def save_speaker_analysis_log(self, audio_path: str, speaker_segments: List[Dict], whisper_result: Dict, application_id: str = None) -> str:
        """í™”ìë¶„ë¦¬ ë° ìŒì„± ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
            log_dir = "logs/speaker_analysis"
            os.makedirs(log_dir, exist_ok=True)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"speaker_analysis_{application_id or 'unknown'}_{timestamp}.json"
            log_path = os.path.join(log_dir, filename)
            
            # ë¶„ì„ ê²°ê³¼ ë°ì´í„° êµ¬ì„±
            analysis_data = {
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "audio_file": audio_path,
                    "application_id": application_id,
                    "analysis_type": "speaker_diarization_and_whisper"
                },
                "speaker_analysis": {
                    "total_speakers": len(set(seg.get('speaker', 'unknown') for seg in speaker_segments)),
                    "speaker_segments": speaker_segments,
                    "applicant_speech_duration": sum(seg.get('duration', 0) for seg in speaker_segments),
                    "total_segments": len(speaker_segments)
                },
                "whisper_analysis": whisper_result,
                "summary": {
                    "transcription_length": len(whisper_result.get("transcription", "")),
                    "speech_rate": whisper_result.get("speech_rate", 0),
                    "analysis_duration": whisper_result.get("duration", 0)
                }
            }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2)
            
            print(f"í™”ìë¶„ë¦¬ ë¶„ì„ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {log_path}")
            return log_path
            
        except Exception as e:
            print(f"í™”ìë¶„ë¦¬ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return None

# í™”ì ë¶„ë¦¬ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
speaker_analysis_service = SpeakerAnalysisService()

# OpenAI API í‚¤ê°€ ìˆì„ ë•Œë§Œ ê·¸ë˜í”„ ì´ˆê¸°í™”
try:
    if os.getenv("OPENAI_API_KEY"):
        graph_agent = build_graph()
        chatbot_graph = create_chatbot_graph()
    else:
        graph_agent = None
        chatbot_graph = None
        print("Warning: OPENAI_API_KEY not found. Some features will be limited.")
except Exception as e:
    print(f"Error initializing agents: {e}")
    graph_agent = None
    chatbot_graph = None

# Redis ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
try:
    redis_monitor = RedisMonitor()
    scheduler = RedisScheduler(redis_monitor)
    print("Redis monitoring system initialized successfully.")
except Exception as e:
    print(f"Error initializing Redis monitor: {e}")
    redis_monitor = None
    scheduler = None

@app.post("/highlight-resume")
async def highlight_resume(request: dict):
    """ì´ë ¥ì„œ í•˜ì´ë¼ì´íŒ… ë¶„ì„ (resume_content ì§ì ‘ ì „ë‹¬)"""
    print(f"ğŸ¯ AI Agent: í•˜ì´ë¼ì´íŒ… ìš”ì²­ ë°›ìŒ!")
    print(f"ğŸ“¥ ìš”ì²­ ë°ì´í„°: {request}")
    
    try:
        # HighlightResumeTool ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        highlight_tool = get_highlight_tool()
        if not highlight_tool:
            print("âŒ HighlightResumeTool ì´ˆê¸°í™” ì‹¤íŒ¨")
            raise HTTPException(status_code=503, detail="HighlightResumeToolì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # application_id í•„ìˆ˜ ì²´í¬
        if "application_id" not in request:
            print("âŒ application_id ëˆ„ë½")
            raise HTTPException(status_code=400, detail="application_id is required")
        
        # resume_content í•„ìˆ˜ ì²´í¬
        if "resume_content" not in request:
            print("âŒ resume_content ëˆ„ë½")
            raise HTTPException(status_code=400, detail="resume_content is required")
        
        application_id = request["application_id"]
        resume_content = request["resume_content"]
        jobpost_id = request.get("jobpost_id")
        company_id = request.get("company_id")
        
        print(f"âœ… íŒŒë¼ë¯¸í„° í™•ì¸ ì™„ë£Œ: application_id={application_id}, jobpost_id={jobpost_id}, company_id={company_id}")
        print(f"ğŸ“„ ì´ë ¥ì„œ ë‚´ìš© ê¸¸ì´: {len(resume_content)} characters")
        
        # resume_content ê¸°ë°˜ í•˜ì´ë¼ì´íŒ… ì‹¤í–‰ (ë¹„ë™ê¸°)
        print("ğŸš€ í•˜ì´ë¼ì´íŒ… ë¶„ì„ ì‹œì‘...")
        result = await highlight_tool.run_all_with_content(
            resume_content=resume_content,
            application_id=application_id,
            jobpost_id=jobpost_id,
            company_id=company_id
        )
        
        print(f"âœ… í•˜ì´ë¼ì´íŒ… ë¶„ì„ ì™„ë£Œ: {len(result.get('highlights', []))} highlights")
        print(f"ğŸ“¤ ì‘ë‹µ ì „ì†¡ ì‹œì‘...")
        print(f"ğŸ“¦ ì‘ë‹µ ë°ì´í„°: {result}")
        return result
        
    except Exception as e:
        print(f"âŒ í•˜ì´ë¼ì´íŒ… ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        print(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/run/")
async def run(request: Request):
    data = await request.json()
    # job_posting, resume í•„ë“œ ë‘˜ ë‹¤ ë°›ì•„ì•¼ í•¨
    job_posting = data.get("job_posting", "")
    resume = data.get("resume", "")
    state = {
        "job_posting": job_posting,
        "resume": resume
    }
    result = graph_agent.invoke(state)
    if result is None:
        return {"error": "LangGraph returned None"}
    return {
        "job_posting_suggestion": result.get("job_posting_suggestion"),
        "resume_score": result.get("resume_score"),
    }

@app.post("/chat/")
async def chat(request: Request):
    """ì±—ë´‡ ëŒ€í™” API"""
    data = await request.json()
    user_message = data.get("message", "")
    session_id = data.get("session_id", None)
    page_context = data.get("page_context", {})  # í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    
    if not user_message:
        return {"error": "Message is required"}
    
    # ì„¸ì…˜ IDê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if not session_id:
        session_id = create_session_id()
    
    # chatbot_graphê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ì‘ë‹µ
    if chatbot_graph is None:
        return {
            "session_id": session_id,
            "ai_response": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì±—ë´‡ ì„œë¹„ìŠ¤ê°€ ì„¤ì • ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "context_used": "",
            "conversation_history_length": 0,
            "page_suggestions": [],
            "dom_actions": [],
            "error": "OpenAI API key not configured"
        }
    
    # ì±—ë´‡ ìƒíƒœ ì´ˆê¸°í™” (í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
    chat_state = initialize_chat_state(user_message, session_id, page_context)
    
    # ì±—ë´‡ ê·¸ë˜í”„ ì‹¤í–‰
    try:
        result = chatbot_graph.invoke(chat_state)
        return {
            "session_id": session_id,
            "ai_response": result.get("ai_response", ""),
            "context_used": result.get("context_used", ""),
            "conversation_history_length": result.get("conversation_history_length", 0),
            "page_suggestions": result.get("page_suggestions", []),  # í˜ì´ì§€ë³„ ì œì•ˆì‚¬í•­
            "dom_actions": result.get("dom_actions", []),  # DOM ì¡°ì‘ ì•¡ì…˜
            "error": result.get("error", "")
        }
    except Exception as e:
        return {
            "session_id": session_id,
            "error": f"Chatbot error: {str(e)}",
            "ai_response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }

@app.post("/chat/add-knowledge/")
async def add_knowledge(request: Request):
    """ì§€ì‹ ë² ì´ìŠ¤ì— ë¬¸ì„œ ì¶”ê°€"""
    data = await request.json()
    documents = data.get("documents", [])
    metadata = data.get("metadata", None)
    
    if not documents:
        return {"error": "Documents are required"}
    
    try:
        chatbot_node = ChatbotNode()
        chatbot_node.add_knowledge(documents, metadata)
        return {"message": f"Added {len(documents)} documents to knowledge base"}
    except Exception as e:
        return {"error": f"Failed to add knowledge: {str(e)}"}

@app.delete("/chat/clear/{session_id}")
async def clear_conversation(session_id: str):
    """íŠ¹ì • ì„¸ì…˜ì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
    try:
        chatbot_node = ChatbotNode()
        chatbot_node.clear_conversation(session_id)
        return {"message": f"Cleared conversation history for session {session_id}"}
    except Exception as e:
        return {"error": f"Failed to clear conversation: {str(e)}"}

@app.get("/chat/session/new")
async def create_new_session():
    """ìƒˆë¡œìš´ ì„¸ì…˜ ìƒì„±"""
    session_id = create_session_id()
    return {"session_id": session_id}

# Redis ëª¨ë‹ˆí„°ë§ ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/monitor/health")
async def get_redis_health():
    """Redis ìƒíƒœ í™•ì¸"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    return redis_monitor.get_health_status()

@app.get("/monitor/sessions")
async def get_session_statistics():
    """ì„¸ì…˜ í†µê³„ ì •ë³´"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    return redis_monitor.get_session_statistics()

@app.post("/monitor/cleanup")
async def cleanup_sessions():
    """ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    return redis_monitor.cleanup_expired_sessions()

@app.post("/monitor/backup")
async def backup_conversations(request: Request):
    """ëŒ€í™” ê¸°ë¡ ë°±ì—…"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    
    data = await request.json()
    backup_name = data.get("backup_name")
    return redis_monitor.backup_conversations(backup_name)

@app.post("/monitor/restore")
async def restore_conversations(request: Request):
    """ëŒ€í™” ê¸°ë¡ ë³µêµ¬"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    
    data = await request.json()
    backup_file = data.get("backup_file")
    if not backup_file:
        return {"error": "backup_file is required"}
    
    return redis_monitor.restore_conversations(backup_file)

@app.get("/monitor/backups")
async def get_backup_list():
    """ë°±ì—… íŒŒì¼ ëª©ë¡"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    return redis_monitor.get_backup_list()

@app.delete("/monitor/backup/{backup_name}")
async def delete_backup(backup_name: str):
    """ë°±ì—… íŒŒì¼ ì‚­ì œ"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    return redis_monitor.delete_backup(backup_name)

@app.post("/monitor/memory-limit")
async def set_memory_limit(request: Request):
    """ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    
    data = await request.json()
    max_memory_mb = data.get("max_memory_mb", 512)
    return redis_monitor.set_memory_limit(max_memory_mb)

@app.post("/monitor/start")
async def start_monitoring():
    """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    
    redis_monitor.start_monitoring()
    return {"message": "Monitoring started"}

@app.post("/monitor/stop")
async def stop_monitoring():
    """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    
    redis_monitor.stop_monitoring()
    return {"message": "Monitoring stopped"}

@app.post("/monitor/auto-cleanup/enable")
async def enable_auto_cleanup():
    """ìë™ ì •ë¦¬ í™œì„±í™”"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    
    redis_monitor.enable_auto_cleanup()
    return {"message": "Auto cleanup enabled"}

@app.post("/monitor/auto-cleanup/disable")
async def disable_auto_cleanup():
    """ìë™ ì •ë¦¬ ë¹„í™œì„±í™”"""
    if redis_monitor is None:
        return {"error": "Redis monitor not initialized"}
    
    redis_monitor.disable_auto_cleanup()
    return {"message": "Auto cleanup disabled"}

@app.post("/monitor/scheduler/start")
async def start_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
    if scheduler is None:
        return {"error": "Scheduler not initialized"}
    
    import asyncio
    asyncio.create_task(scheduler.start())
    return {"message": "Scheduler started"}

@app.post("/monitor/scheduler/stop")
async def stop_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
    if scheduler is None:
        return {"error": "Scheduler not initialized"}
    
    return {"message": "Scheduler stopped"}

@app.get("/monitor/scheduler/status")
async def get_scheduler_status():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸"""
    if scheduler is None:
        return {"error": "Scheduler not initialized"}
    
    return scheduler.get_scheduler_status()

@app.post("/monitor/cleanup/manual")
async def manual_cleanup():
    """ìˆ˜ë™ ì •ë¦¬ ì‹¤í–‰"""
    if scheduler is None:
        return {"error": "Scheduler not initialized"}
    
    result = await scheduler.run_manual_cleanup()
    return result

@app.post("/monitor/backup/manual")
async def manual_backup(request: Request):
    """ìˆ˜ë™ ë°±ì—… ì‹¤í–‰"""
    if scheduler is None:
        return {"error": "Scheduler not initialized"}
    
    data = await request.json()
    backup_name = data.get("backup_name")
    
    result = await scheduler.run_manual_backup(backup_name)
    return result



@app.post("/extract-weights/")
async def extract_weights(request: Request):
    """ì±„ìš©ê³µê³  ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    data = await request.json()
    job_posting_content = data.get("job_posting", "")
    
    if not job_posting_content:
        return {"error": "Job posting content is required"}
    
    try:
        state = {"job_posting": job_posting_content}
        result = weight_extraction_tool(state)
        weights = result.get("weights", [])
        
        return {
            "weights": weights,
            "message": f"Successfully extracted {len(weights)} weights"
        }
    except Exception as e:
        return {
            "error": f"Failed to extract weights: {str(e)}",
            "weights": []
        }

@app.post("/evaluate-application/")
async def evaluate_application_api(request: Request):
    """ì§€ì›ìì˜ ì„œë¥˜ë¥¼ AIë¡œ í‰ê°€í•©ë‹ˆë‹¤."""
    data = await request.json()
    job_posting = data.get("job_posting", "")
    spec_data = data.get("spec_data", {})
    resume_data = data.get("resume_data", {})
    weight_data = data.get("weight_data", {})
    
    if not job_posting or not spec_data or not resume_data:
        return {"error": "job_posting, spec_data, and resume_data are required"}
    
    try:
        # weight_dataë¥¼ í¬í•¨í•˜ì—¬ í‰ê°€ ì‹¤í–‰
        initial_state = {
            "job_posting": job_posting,
            "spec_data": spec_data,
            "resume_data": resume_data,
            "weight_data": weight_data,
            "ai_score": 0.0,
            "scoring_details": {},
            "pass_reason": "",
            "fail_reason": "",
            "status": "",
            "decision_reason": "",
            "confidence": 0.0
        }
        
        result = evaluate_application(job_posting, spec_data, resume_data, weight_data)
        
        return {
            "ai_score": result.get("ai_score", 0.0),
            "document_status": result.get("document_status", "REJECTED"),
            "pass_reason": result.get("pass_reason", ""),
            "fail_reason": result.get("fail_reason", ""),
            "scoring_details": result.get("scoring_details", {}),
            "decision_reason": result.get("decision_reason", ""),
            "confidence": result.get("confidence", 0.0),
            "message": "Application evaluation completed successfully"
        }
    except Exception as e:
        return {
            "error": f"Failed to evaluate application: {str(e)}",
            "ai_score": 0.0,
            "document_status": "REJECTED",
            "pass_reason": "",
            "fail_reason": "",
            "scoring_details": {},
            "decision_reason": "",
            "confidence": 0.0
        }

# í¼ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.post("/ai/form-fill")
async def ai_form_fill(request: Request):
    """AIë¥¼ í†µí•œ í¼ ìë™ ì±„ìš°ê¸°"""
    data = await request.json()
    description = data.get("description", "")
    current_form_data = data.get("current_form_data", {})
    
    if not description:
        return {"error": "Description is required"}
    
    try:
        state = {
            "description": description,
            "current_form_data": current_form_data
        }
        result = form_fill_tool(state)
        return result
    except Exception as e:
        return {"error": f"Form fill failed: {str(e)}"}

@app.post("/ai/form-improve")
async def ai_form_improve(request: Request):
    """AIë¥¼ í†µí•œ í¼ ê°œì„  ì œì•ˆ"""
    data = await request.json()
    current_form_data = data.get("current_form_data", {})
    
    if not current_form_data:
        return {"error": "Current form data is required"}
    
    try:
        state = {
            "current_form_data": current_form_data
        }
        result = form_improve_tool(state)
        return result
    except Exception as e:
        return {"error": f"Form improve failed: {str(e)}"}

@app.post("/ai/form-field-update")
async def ai_form_field_update(request: Request):
    """AIë¥¼ í†µí•œ íŠ¹ì • í¼ í•„ë“œ ìˆ˜ì •"""
    data = await request.json()
    field_name = data.get("field_name", "")
    new_value = data.get("new_value", "")
    current_form_data = data.get("current_form_data", {})
    
    if not field_name or not new_value:
        return {"error": "Field name and new value are required"}
    
    try:
        state = {
            "field_name": field_name,
            "new_value": new_value,
            "current_form_data": current_form_data
        }
        result = form_edit_tool(state)
        return result
    except Exception as e:
        return {"error": f"Form field update failed: {str(e)}"}

@app.post("/ai/form-status-check")
async def ai_form_status_check(request: Request):
    """AIë¥¼ í†µí•œ í¼ ìƒíƒœ í™•ì¸"""
    data = await request.json()
    current_form_data = data.get("current_form_data", {})
    
    try:
        state = {
            "current_form_data": current_form_data
        }
        result = form_status_check_tool(state)
        return result
    except Exception as e:
        return {"error": f"Form status check failed: {str(e)}"}

@app.post("/ai/field-improve")
async def ai_field_improve(request: Request):
    """AIë¥¼ í†µí•œ íŠ¹ì • í•„ë“œ ê°œì„ """
    data = await request.json()
    field_name = data.get("field_name", "")
    current_content = data.get("current_content", "")
    user_request = data.get("user_request", "")
    form_context = data.get("form_context", {})
    
    if not field_name:
        return {"error": "Field name is required"}
    
    try:
        state = {
            "field_name": field_name,
            "current_content": current_content,
            "user_request": user_request,
            "form_context": form_context
        }
        result = form_improve_tool(state)
        return result
    except Exception as e:
        return {"error": f"Field improve failed: {str(e)}"}

@app.post("/ai/route")
async def ai_route(request: Request):
    """LLM ê¸°ë°˜ ë¼ìš°íŒ… - ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ë¡œ ë¶„ê¸°"""
    data = await request.json()
    message = data.get("message", "")
    current_form_data = data.get("current_form_data", {})
    user_intent = data.get("user_intent", "")
    
    print(f"ğŸ”„ /ai/route í˜¸ì¶œ: message={message}")
    
    if not message:
        return {"error": "message is required"}
    
    try:
        # LangGraphë¥¼ ì‚¬ìš©í•œ ë¼ìš°íŒ…
        state = {
            "message": message,
            "user_intent": user_intent,
            "current_form_data": current_form_data,
            "description": message,  # form_fill_toolì´ description í•„ë“œë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì¶”ê°€
            "page_context": data.get("page_context", {})
        }
        
        # ê·¸ë˜í”„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        if graph_agent is None:
            return {"error": "Graph agent not initialized"}
        
        result = graph_agent.invoke(state)
        print(f"ğŸ¯ ë¼ìš°íŒ… ê²°ê³¼: {result}")
        
        # ê²°ê³¼ì—ì„œ ì ì ˆí•œ ì‘ë‹µ ì¶”ì¶œ
        if "info" in result:
            print(f"ğŸ“‹ info_tool ê²°ê³¼ ê°ì§€: {result['info']}")
            return {
                "success": True,
                "response": result["info"],
                "tool_used": "info_tool"
            }
        elif "form_data" in result:
            return {
                "success": True,
                "response": result.get("message", "í¼ì´ ì±„ì›Œì¡ŒìŠµë‹ˆë‹¤."),
                "form_data": result.get("form_data", {}),
                "tool_used": "form_fill_tool"
            }
        elif "suggestions" in result:
            return {
                "success": True,
                "response": "í¼ ê°œì„  ì œì•ˆ:\n" + "\n".join([f"{i+1}. {s}" for i, s in enumerate(result.get("suggestions", []))]),
                "tool_used": "form_improve_tool"
            }
        elif "questions" in result:
            return {
                "success": True,
                "response": "ë©´ì ‘ ì§ˆë¬¸:\n" + "\n".join([f"{i+1}. {q}" for i, q in enumerate(result.get("questions", []))]),
                "tool_used": "project_question_generator"
            }
        elif "status" in result:
            return {
                "success": True,
                "response": result.get("status", "í¼ ìƒíƒœë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤."),
                "tool_used": "form_status_check_tool"
            }
        elif "response" in result:
            # spell_check_tool ë“±ì´ ë°˜í™˜í•˜ëŠ” response í•„ë“œ ì²˜ë¦¬
            return {
                "success": True,
                "response": result.get("response", "ìš”ì²­ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤."),
                "tool_used": "spell_check_tool"
            }
        else:
            # messageê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
            response_message = result.get("message", "ìš”ì²­ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
            print(f"ğŸ“ ê¸°ë³¸ ì‘ë‹µ: {response_message}")
            return {
                "success": True,
                "response": response_message,
                "form_data": result.get("form_data", {}),
                "tool_used": "unknown"
            }
            
    except Exception as e:
        print(f"âŒ /ai/route ì˜¤ë¥˜: {e}")
        return {"success": False, "error": str(e)}

@app.post("/chat/suggest-questions")
async def suggest_questions(request: Request):
    """LLMì„ í™œìš©í•œ ì˜ˆì‹œ ì§ˆë¬¸(ë¹ ë¥¸ ì‘ë‹µ) ìƒì„± API"""
    data = await request.json()
    recent_messages = data.get("recent_messages", [])  # [{sender, text, timestamp} ...]
    page_context = data.get("page_context", {})
    form_data = data.get("form_data", {})

    # ìµœê·¼ ë©”ì‹œì§€ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    last_user_message = ""
    for msg in reversed(recent_messages):
        if msg.get("sender") == "user":
            last_user_message = msg.get("text", "")
            break

    # í”„ë¡¬í”„íŠ¸ ì„¤ê³„
    prompt = f"""
    ì•„ë˜ëŠ” ì±„ìš©/HR ì±—ë´‡ì˜ ëŒ€í™” ë§¥ë½ê³¼ í˜ì´ì§€ ì •ë³´, í¼ ìƒíƒœì…ë‹ˆë‹¤.
    ì´ ë§¥ë½ì—ì„œ ì‚¬ìš©ìê°€ ë‹¤ìŒì— í•  ìˆ˜ ìˆëŠ” ìœ ìš©í•œ ì˜ˆì‹œ ì§ˆë¬¸(ë¹ ë¥¸ ì‘ë‹µ ë²„íŠ¼ìš©)ì„ 4ê°œ ì¶”ì²œí•´ ì£¼ì„¸ìš”.
    - ë„ˆë¬´ ë‹¨ìˆœí•˜ê±°ë‚˜ ë°˜ë³µì ì´ì§€ ì•Šê²Œ, ì‹¤ì œë¡œ ë„ì›€ì´ ë  ë§Œí•œ ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    - ì˜ˆì‹œ ì§ˆë¬¸ì€ í•œê¸€ë¡œ, ì§§ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
    - ë°˜ë“œì‹œ ë°°ì—´(JSON)ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

    [ìµœê·¼ ì‚¬ìš©ì ë©”ì‹œì§€]
    {last_user_message}

    [í˜ì´ì§€ ì •ë³´]
    {page_context}

    [í¼ ìƒíƒœ]
    {form_data}

    ì˜ˆì‹œ ì‘ë‹µ:
    ["ì§€ì›ì ëª©ë¡ ë³´ì—¬ì¤˜", "ê²½ë ¥ ìš°ëŒ€ ì¡°ê±´ ì¶”ê°€", "ë©´ì ‘ ì¼ì • ì¶”ì²œí•´ì¤˜", "í¼ ê°œì„  ì œì•ˆ"]
    """
    
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)
    try:
        response = llm.invoke(prompt)
        text = response.content.strip()
        # JSON ë°°ì—´ë§Œ ì¶”ì¶œ
        if "[" in text:
            start = text.find("[")
            end = text.find("]", start)
            arr = text[start:end+1]
            suggestions = json.loads(arr)
        else:
            suggestions = [text]
        return {"suggestions": suggestions}
    except Exception as e:
        return {"suggestions": ["ì§€ì›ì ëª©ë¡ ë³´ì—¬ì¤˜", "í¼ ê°œì„  ì œì•ˆ", "ë©´ì ‘ ì¼ì • ì¶”ì²œí•´ì¤˜", "ì±„ìš©ê³µê³  ì‘ì„± ë°©ë²•"]}

@app.post("/agent/speech-recognition")
async def speech_recognition_api(request: Request):
    """ì¸ì‹ API"""
    data = await request.json()
    audio_file_path = data.get("audio_file_path", "") 
    if not audio_file_path:
        return {"error": "audio_file_path is required"}
    
    try:
        # ìŒì„± ì¸ì‹ ë„êµ¬ ì‹¤í–‰
        state = {
            "audio_file_path": audio_file_path
        }
        
        result = speech_recognition_tool(state)
        
        return {
            "success": True,
            "speech_analysis": result.get("speech_analysis", {})
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/agent/realtime-interview-evaluation")
async def realtime_interview_evaluation_api(request: Request):
    """ì‹¤ì‹œê°„ ë©´ì ‘ í‰ê°€ API"""
    data = await request.json()
    transcription = data.get("transcription", "")
    speakers = data.get("speakers", [])
    job_info = data.get("job_info", {})
    resume_info = data.get("resume_info", {})
    current_time = data.get("current_time", 0)
    
    if not transcription:
        return {"error": "transcription is required"}
    
    try:
        # ì‹¤ì‹œê°„ í‰ê°€ ë„êµ¬ ì‹¤í–‰
        state = {
            "transcription": transcription,
            "speakers": speakers,
            "job_info": job_info,
            "resume_info": resume_info,
            "current_time": current_time
        }
        
        # ì‹¤ì‹œê°„ í‰ê°€ ë„êµ¬ë¥¼ ë™ì ìœ¼ë¡œ import
        from tools.realtime_interview_evaluation_tool import realtime_interview_evaluation_tool
        result = realtime_interview_evaluation_tool(state)
        
        return {
            "success": True,
            "realtime_evaluation": result.get("realtime_evaluation", {})
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/agent/ai-interview-evaluation")
async def ai_interview_evaluation_api(request: Request):
    """AI ë©´ì ‘ í‰ê°€ API"""
    data = await request.json()
    session_id = data.get("session_id")
    job_info = data.get("job_info", "")
    audio_data = data.get("audio_data", {})
    behavior_data = data.get("behavior_data", {})
    game_data = data.get("game_data", {})
    
    if not session_id:
        return {"error": "session_id is required"}
    
    try:
        # AI ë©´ì ‘ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        from agents.ai_interview_workflow import run_ai_interview
        
        result = run_ai_interview(
            session_id=session_id,
            job_info=job_info,
            audio_data=audio_data,
            behavior_data=behavior_data,
            game_data=game_data
        )
        
        return {
            "success": True,
            "total_score": result.get("total_score", 0),
            "evaluation_metrics": result.get("evaluation_metrics", {}),
            "feedback": result.get("feedback", []),
            "session_id": session_id
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/speaker-analysis-and-trim")
async def speaker_analysis_and_trim(request: SpeakerAnalysisRequest):
    """í™”ì ë¶„ë¦¬ ë° ë¹„ë””ì˜¤ ìë¥´ê¸° ì—”ë“œí¬ì¸íŠ¸"""
    try:
        print("í™”ì ë¶„ë¦¬ ë° ë¹„ë””ì˜¤ ìë¥´ê¸° ìš”ì²­ ì‹œì‘...")
        
        # base64 ë””ì½”ë”©
        audio_data = base64.b64decode(request.audio_data)
        video_data = base64.b64decode(request.video_data)
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        temp_audio_path = tempfile.mktemp(suffix=".wav")
        temp_video_path = tempfile.mktemp(suffix=".mp4")
        
        try:
            # íŒŒì¼ ì €ì¥
            with open(temp_audio_path, 'wb') as f:
                f.write(audio_data)
            with open(temp_video_path, 'wb') as f:
                f.write(video_data)
            
            print(f"ì„ì‹œ íŒŒì¼ ìƒì„±: {temp_audio_path}, {temp_video_path}")
            
            # 1ë‹¨ê³„: í™”ì ë¶„ë¦¬
            applicant_segments = speaker_analysis_service.extract_applicant_audio(temp_audio_path)
            
            # 2ë‹¨ê³„: ë©´ì ‘ì ë°œí™” ì‹œê°„ ê³„ì‚°
            applicant_speech_duration = sum(seg['duration'] for seg in applicant_segments)
            
            # 3ë‹¨ê³„: ì§ˆë¬¸ë³„ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ (10ë¶„ ì´ìƒ ì˜ìƒì¸ ê²½ìš°)
            video_duration = speaker_analysis_service._get_video_duration(temp_video_path)
            video_segments = []
            
            if video_duration and video_duration > 600:  # 10ë¶„ ì´ìƒ
                print(f"ê¸´ ì˜ìƒ ê°ì§€ ({video_duration:.1f}ì´ˆ), ì§ˆë¬¸ë³„ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ ì‹œì‘...")
                video_segments = speaker_analysis_service.create_video_segments_by_questions(
                    temp_video_path, temp_audio_path, max_segment_duration=60
                )
                print(f"ì§ˆë¬¸ë³„ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¦¬ ì™„ë£Œ: {len(video_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            else:
                print(f"ì§§ì€ ì˜ìƒ ({video_duration:.1f}ì´ˆ), ê¸°ë³¸ ìë¥´ê¸° ì‚¬ìš©")
            
            # 4ë‹¨ê³„: ê¸°ë³¸ ë¹„ë””ì˜¤ ìë¥´ê¸° (ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ëŠ” ê²½ìš°)
            trimmed_video_path = temp_video_path
            if not video_segments:
                trimmed_video_path = speaker_analysis_service.trim_video_by_applicant_speech(
                    temp_video_path, applicant_segments, max_duration=30
                )
            
            # 5ë‹¨ê³„: Whisper ë¶„ì„
            whisper_analysis = speaker_analysis_service.analyze_audio_with_whisper(temp_audio_path)
            
            # 6ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ë¡œê·¸ ì €ì¥
            log_path = speaker_analysis_service.save_speaker_analysis_log(
                temp_audio_path, applicant_segments, whisper_analysis
            )
            
            # 7ë‹¨ê³„: ê²°ê³¼ ì •ë¦¬
            trimmed_video_base64 = None
            if trimmed_video_path != temp_video_path and os.path.exists(trimmed_video_path):
                # ìë¥¸ ë¹„ë””ì˜¤ë¥¼ base64ë¡œ ì¸ì½”ë”©
                with open(trimmed_video_path, 'rb') as f:
                    trimmed_video_data = f.read()
                trimmed_video_base64 = base64.b64encode(trimmed_video_data).decode('utf-8')
            
            analysis_result = {
                "applicant_segments": applicant_segments,
                "applicant_speech_duration": applicant_speech_duration,
                "trimmed_video_base64": trimmed_video_base64,
                "is_trimmed": trimmed_video_path != temp_video_path,
                "whisper_analysis": whisper_analysis,
                "trimmed_filename": f"trimmed_{request.video_filename}" if trimmed_video_base64 else None,
                "log_path": log_path,
                "video_segments": video_segments,  # ì§ˆë¬¸ë³„ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´
                "original_duration": video_duration,
                "segment_count": len(video_segments)
            }
            
            print("í™”ì ë¶„ë¦¬ ë° ë¹„ë””ì˜¤ ìë¥´ê¸° ì™„ë£Œ")
            
            return {
                "success": True,
                "message": "í™”ì ë¶„ë¦¬ ë° ë¹„ë””ì˜¤ ìë¥´ê¸°ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
                "analysis": analysis_result
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                if os.path.exists(temp_audio_path):
                    os.remove(temp_audio_path)
                if os.path.exists(temp_video_path):
                    os.remove(temp_video_path)
                print("ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                print(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì˜¤ë¥˜: {str(e)}")
        
    except Exception as e:
        print(f"í™”ì ë¶„ë¦¬ ë° ë¹„ë””ì˜¤ ìë¥´ê¸° ì˜¤ë¥˜: {str(e)}")
        return {
            "success": False,
            "message": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "analysis": {}
        }

@app.post("/whisper-analysis")
async def whisper_analysis_api(request: WhisperAnalysisRequest):
    """Whisper ìŒì„± ë¶„ì„ API"""
    audio_path = request.audio_path
    application_id = request.application_id

    print(f"ğŸ¤ Whisper ë¶„ì„ API í˜¸ì¶œ: audio_path={audio_path}, application_id={application_id}")

    if not audio_path:
        print("âŒ audio_pathê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return {"success": False, "error": "audio_path is required"}

    try:
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(audio_path):
            print(f"âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {audio_path}")
            return {
                "success": False, 
                "error": f"Audio file not found: {audio_path}"
            }

        print(f"âœ… ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¸ë¨: {audio_path}")
        
        # Whisper ë¶„ì„ ì‹¤í–‰
        whisper_analysis = speaker_analysis_service.analyze_audio_with_whisper(audio_path)
        
        if not whisper_analysis:
            print("âŒ Whisper ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {
                "success": False,
                "error": "Whisper analysis failed"
            }

        print(f"âœ… Whisper ë¶„ì„ ì™„ë£Œ: {whisper_analysis.get('text', '')[:100]}...")
        
        # ë¡œê·¸ ì €ì¥
        log_path = speaker_analysis_service.save_speaker_analysis_log(
            audio_path, [], whisper_analysis, application_id
        )

        return {
            "success": True,
            "whisper_analysis": whisper_analysis,
            "log_path": log_path
        }
    except Exception as e:
        print(f"âŒ Whisper ë¶„ì„ API ì˜¤ë¥˜: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/diarized-qa-analysis")
async def diarized_qa_analysis(request: QAAnalysisRequest):
    """í™”ìë¶„ë¦¬ë¡œ Qâ†’A êµ¬ê°„ì„ ë§Œë“¤ê³  ì§€ì›ì ë‹µë³€ë³„ Whisper ë¶„ì„ ë°˜í™˜"""
    audio_path = request.audio_path
    application_id = request.application_id
    try:
        if not audio_path or not os.path.exists(audio_path):
            return {"success": False, "error": f"audio not found: {audio_path}"}
        result = speaker_analysis_service.build_qa_pairs_and_analyze_answers(
            audio_path,
            persist=bool(request.persist),
            output_dir=request.output_dir,
            application_id=str(application_id) if application_id else None,
            max_workers=int(request.max_workers or 2)
        )
        
        # ê°ì •/ë¬¸ë§¥ ë¶„ì„ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš° ì¶”ê°€ ë¶„ì„ ìˆ˜í–‰
        if request.run_emotion_context and result.get("success"):
            try:
                print("ğŸ­ ê°ì •/ë¬¸ë§¥ ë¶„ì„ ì‹œì‘...")
                
                # ì „ì²´ ì „ì‚¬ë³¸ ìˆ˜ì§‘
                full_transcription = ""
                for qa in result.get("qa", []):
                    if qa.get("answer_transcription"):
                        full_transcription += qa["answer_transcription"] + " "
                
                if full_transcription.strip():
                    # ê°ì • ë¶„ì„
                    emotion_result = await emotion_analysis_api({"transcription": full_transcription})
                    if emotion_result.get("success"):
                        result["emotion_analysis"] = emotion_result["analysis"]
                        print("âœ… ê°ì • ë¶„ì„ ì™„ë£Œ")
                    
                    # ë¬¸ë§¥ ë¶„ì„
                    context_result = await openai_context_analysis_api({
                        "transcription": full_transcription,
                        "speakers": result.get("speakers", [])
                    })
                    if context_result.get("success"):
                        result["context_analysis"] = context_result["analysis"]
                        print("âœ… ë¬¸ë§¥ ë¶„ì„ ì™„ë£Œ")
                
                print("ğŸ­ ê°ì •/ë¬¸ë§¥ ë¶„ì„ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âš ï¸ ê°ì •/ë¬¸ë§¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {str(e)}")
                # ê°ì •/ë¬¸ë§¥ ë¶„ì„ ì‹¤íŒ¨í•´ë„ QA ë¶„ì„ ê²°ê³¼ëŠ” ë°˜í™˜
        # ë¡œê·¸ ì €ì¥(ì„ íƒ)
        try:
            speaker_analysis_service.save_speaker_analysis_log(
                audio_path, [], {"qa_result": result}, str(application_id) if application_id else None
            )
        except Exception:
            pass
        # ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚­ì œ ì˜µì…˜ ì²˜ë¦¬
        try:
            if bool(request.delete_after_input) and os.path.exists(audio_path):
                os.remove(audio_path)
        except Exception as e:
            print(f"ì…ë ¥ ì˜¤ë””ì˜¤ ì‚­ì œ ì˜¤ë¥˜: {str(e)}")

        return result
    except Exception as e:
        return {"success": False, "error": str(e), "qa": []}

@app.post("/openai-answer-analysis")
async def openai_answer_analysis_api(request: dict):
    """OpenAI ë‹µë³€ í’ˆì§ˆ ë¶„ì„ API"""
    try:
        from tools.openai_nlp_analyzer import openai_nlp_analyzer
        
        question = request.get("question", "")
        answer = request.get("answer", "")
        
        if not question or not answer:
            return {"success": False, "error": "question and answer are required"}
        
        analysis = openai_nlp_analyzer.analyze_answer_quality(question, answer)
        
        return {
            "success": True,
            "analysis": analysis
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/openai-context-analysis")
async def openai_context_analysis_api(request: dict):
    """OpenAI ë¬¸ë§¥ ë¶„ì„ API"""
    try:
        from tools.openai_nlp_analyzer import openai_nlp_analyzer
        
        transcription = request.get("transcription", "")
        speakers = request.get("speakers", [])
        
        if not transcription:
            return {"success": False, "error": "transcription is required"}
        
        analysis = openai_nlp_analyzer.analyze_interview_context(transcription, speakers)
        
        return {
            "success": True,
            "analysis": analysis
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/emotion-analysis")
async def emotion_analysis_api(request: dict):
    """OpenAI ê°ì • ë¶„ì„ API"""
    try:
        from tools.openai_nlp_analyzer import openai_nlp_analyzer
        
        transcription = request.get("transcription", "")
        
        if not transcription:
            return {"success": False, "error": "transcription is required"}
        
        analysis = openai_nlp_analyzer.analyze_emotion_from_text(transcription)
        
        return {
            "success": True,
            "analysis": analysis
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

@app.post("/evaluate-audio")
async def evaluate_audio(
    application_id: int = Form(...),
    question_id: int = Form(...),
    question_text: str = Form(...),
    audio_file: UploadFile = File(...)
):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë°›ì•„ ì‹¤ì‹œê°„ìœ¼ë¡œ STT, ê°ì •/íƒœë„, ë‹µë³€ ì ìˆ˜í™” ê²°ê³¼ë¥¼ ë°˜í™˜
    """
    # 1. ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(await audio_file.read())
        tmp_path = tmp.name

    try:
        # 2. ì˜¤ë””ì˜¤â†’í…ìŠ¤íŠ¸(STT)
        speech_tool = SpeechRecognitionTool()
        trans_result = speech_tool.transcribe_audio(tmp_path)
        trans_text = trans_result.get("text", "")

        # 3. ê°ì •/íƒœë„ ë¶„ì„
        realtime_tool = RealtimeInterviewEvaluationTool()
        eval_result = realtime_tool._evaluate_realtime_content(trans_text, "applicant", 0)
        sentiment = eval_result.get("sentiment", "neutral")
        if sentiment == "positive":
            emotion = attitude = "ê¸ì •"
        elif sentiment == "negative":
            emotion = attitude = "ë¶€ì •"
        else:
            emotion = attitude = "ë³´í†µ"

        # 4. ë‹µë³€ ì ìˆ˜í™”
        grade = grade_written_test_answer(question_text, trans_text)
        answer_score = grade.get("score")
        answer_feedback = grade.get("feedback")

        return {
            "answer_text_transcribed": trans_text,
            "emotion": emotion,
            "attitude": attitude,
            "answer_score": answer_score,
            "answer_feedback": answer_feedback,
        }
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

@app.post("/realtime-audio-analysis")
async def realtime_audio_analysis(
    audio_data: str = Form(...),  # base64 encoded audio chunk
    session_id: str = Form(...),
    timestamp: float = Form(...),
    application_id: str = Form(None)
):
    """ì‹¤ì‹œê°„ ìŒì„± ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        print(f"ì‹¤ì‹œê°„ ìŒì„± ë¶„ì„ ìš”ì²­: session_id={session_id}, timestamp={timestamp}")
        
        # base64 ë””ì½”ë”©
        audio_chunk = base64.b64decode(audio_data)
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        temp_audio_path = tempfile.mktemp(suffix=".wav")
        
        try:
            # ì˜¤ë””ì˜¤ ì²­í¬ ì €ì¥
            with open(temp_audio_path, 'wb') as f:
                f.write(audio_chunk)
            
            # 1ë‹¨ê³„: í™”ì ë¶„ë¦¬ (ì‹¤ì‹œê°„ ë²„ì „)
            speaker_segments = speaker_analysis_service.extract_applicant_audio(temp_audio_path)
            
            # 2ë‹¨ê³„: Whisper ë¶„ì„
            whisper_analysis = speaker_analysis_service.analyze_audio_with_whisper(temp_audio_path)
            
            # 3ë‹¨ê³„: ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼ ë¡œê·¸ ì €ì¥
            log_path = speaker_analysis_service.save_speaker_analysis_log(
                temp_audio_path, speaker_segments, whisper_analysis, application_id
            )
            
            # 4ë‹¨ê³„: ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
            analysis_result = {
                "session_id": session_id,
                "timestamp": timestamp,
                "speaker_segments": speaker_segments,
                "whisper_analysis": whisper_analysis,
                "log_path": log_path,
                "analysis_duration": whisper_analysis.get("duration", 0),
                "speech_rate": whisper_analysis.get("speech_rate", 0),
                "transcription": whisper_analysis.get("transcription", "")
            }
            
            print(f"ì‹¤ì‹œê°„ ìŒì„± ë¶„ì„ ì™„ë£Œ: {len(speaker_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸, {whisper_analysis.get('speech_rate', 0):.2f} wpm")
            
            return {
                "success": True,
                "result": analysis_result
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists(temp_audio_path):
                os.remove(temp_audio_path)
                
    except Exception as e:
        print(f"ì‹¤ì‹œê°„ ìŒì„± ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=8001, 
        reload=False,  # ìë™ ë¦¬ë¡œë“œ ë¹„í™œì„±í™”
        log_level="info"
    )
