from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from typing import Dict, Any, List, Optional
import json
import re
from agent.utils.llm_cache import redis_cache

# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)

# ì„ë² ë”© ì‹œìŠ¤í…œ ê´€ë ¨ ì½”ë“œ ì™„ì „ ì œê±°

def is_transition_word(text: str) -> bool:
    """ì „í™˜ì–´ ê°ì§€ í•¨ìˆ˜"""
    transition_patterns = [
        # ëŒ€ì¡°/ë°˜ì „ ì „í™˜ì–´
        r'í•˜ì§€ë§Œ|ê·¸ëŸ¼ì—ë„\s*ë¶ˆêµ¬í•˜ê³ |ê·¸ëŸ¬ë‚˜|ë‹¤ë§Œ|ë‹¨|ì˜¤íˆë ¤|ë°˜ë©´|ë°˜ëŒ€ë¡œ|ëŒ€ì‹ |ëŒ€ì‹ ì—|ê·¸ëŸ°ë°|ê·¸ë ‡ì§€ë§Œ',
        # ì‹œê°„/ìˆœì„œ ì „í™˜ì–´
        r'ê·¸ëŸ¬ë‹¤ê°€|ê·¸\s*í›„|ì´í›„|ê·¸\s*ë‹¤ìŒ|ë‹¤ìŒì—ëŠ”|ê·¸\s*ë•Œë¶€í„°|ê·¸\s*ë•Œ|ê·¸\s*ì´í›„|ê·¸\s*ë‹¤ìŒì—',
        # ì¡°ê±´/ê²°ê³¼ ì „í™˜ì–´
        r'ë§Œì•½|ë§Œì•½ì—|ê²°ê³¼ì ìœ¼ë¡œ|ê²°êµ­|ë§ˆì¹¨ë‚´|ë“œë””ì–´|ê·¸\s*ê²°ê³¼|ê·¸\s*ëì—',
        # ì¶”ê°€/ê°•ì¡° ì „í™˜ì–´
        r'ë˜í•œ|ê²Œë‹¤ê°€|ë”ìš±ì´|ë¬´ì—‡ë³´ë‹¤|íŠ¹íˆ|íŠ¹ë³„íˆ|ë”êµ¬ë‚˜|ê±°ê¸°ì—|ë˜\s*í•œí¸',
        # ì¸ê³¼ ì „í™˜ì–´
        r'ê·¸\s*ì´ìœ ë¡œ|ê·¸\s*ë•Œë¬¸ì—|ê·¸\s*ë˜ì„œ|ê·¸\s*ë•Œë¬¸|ê·¸\s*ê²°ê³¼ë¡œ|ê·¸\s*ë•ë¶„ì—',
        # ì˜ˆì‹œ ì „í™˜ì–´
        r'ì˜ˆë¥¼\s*ë“¤ë©´|ì˜ˆì‹œë¡œ|êµ¬ì²´ì ìœ¼ë¡œ|ì‹¤ì œë¡œ|ì‚¬ì‹¤|ì‹¤ì œë¡œëŠ”'
    ]
    
    for pattern in transition_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False

def filter_negative_highlights_with_transitions(highlights: List[Dict[str, Any]], full_text: str) -> List[Dict[str, Any]]:
    """ì „í™˜ì–´ë¥¼ ê³ ë ¤í•˜ì—¬ ë¶€ì • í•˜ì´ë¼ì´íŒ…ì„ í•„í„°ë§"""
    if not highlights:
        return highlights
    
    filtered_highlights = []
    
    for highlight in highlights:
        sentence = highlight.get('sentence', '')
        category = highlight.get('category', '')
        
        # ë¶€ì • ê´€ë ¨ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§
        if category in ['negative_tone', 'mismatch']:
            # ì „í™˜ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ì¸ì§€ í™•ì¸
            if is_transition_word(sentence):
                # ì „í™˜ì–´ ì¤‘ì‹¬ ë¬¸ë§¥ ë¶„ì„
                context_analysis = analyze_transition_context(sentence)
                
                if context_analysis['has_context_change']:
                    if context_analysis['positive_after_negative']:
                        print(f"ì „í™˜ì–´ ë¬¸ë§¥ ë³€í™” ê°ì§€ (ë¶€ì •â†’ê¸ì •) - ë¶€ì • í•˜ì´ë¼ì´íŒ… ì œì™¸: {sentence[:50]}...")
                        continue  # ì´ í•˜ì´ë¼ì´íŒ…ì€ ì œì™¸
                    elif context_analysis['negative_after_positive']:
                        print(f"ì „í™˜ì–´ ë¬¸ë§¥ ë³€í™” ê°ì§€ (ê¸ì •â†’ë¶€ì •) - ë¶€ì • í•˜ì´ë¼ì´íŒ… ìœ ì§€: {sentence[:50]}...")
                        # ë¶€ì • í•˜ì´ë¼ì´íŒ… ìœ ì§€ (ê¸°ë³¸ ë™ì‘)
        
        filtered_highlights.append(highlight)
    
    return filtered_highlights

def analyze_transition_context(sentence: str) -> Dict[str, Any]:
    """ì „í™˜ì–´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì•ë’¤ ë¬¸ë§¥ì„ ë¶„ì„"""
    result = {
        'has_context_change': False,
        'positive_after_negative': False,
        'negative_after_positive': False,
        'transition_word': '',
        'before_transition': '',
        'after_transition': '',
        'before_sentiment': 'neutral',
        'after_sentiment': 'neutral'
    }
    
    # ì „í™˜ì–´ ìœ„ì¹˜ ì°¾ê¸°
    transition_patterns = [
        r'í•˜ì§€ë§Œ|ê·¸ëŸ¬ë‚˜|ê·¸ëŸ°ë°|ê·¸ë ‡ì§€ë§Œ|ë‹¤ë§Œ|ë‹¨|ì˜¤íˆë ¤|ë°˜ë©´|ë°˜ëŒ€ë¡œ|ëŒ€ì‹ |ëŒ€ì‹ ì—',
        r'ê·¸ëŸ¬ë‹¤ê°€|ê·¸\s*í›„|ì´í›„|ê·¸\s*ë‹¤ìŒ|ë‹¤ìŒì—ëŠ”|ê·¸\s*ë•Œë¶€í„°',
        r'ë§Œì•½|ë§Œì•½ì—|ê²°ê³¼ì ìœ¼ë¡œ|ê²°êµ­|ë§ˆì¹¨ë‚´|ë“œë””ì–´',
        r'ë˜í•œ|ê²Œë‹¤ê°€|ë”ìš±ì´|ë¬´ì—‡ë³´ë‹¤|íŠ¹íˆ|íŠ¹ë³„íˆ'
    ]
    
    for pattern in transition_patterns:
        match = re.search(pattern, sentence, re.IGNORECASE)
        if match:
            transition_word = match.group()
            before_text = sentence[:match.start()].strip()
            after_text = sentence[match.end():].strip()
            
            # ì „í™˜ì–´ ì•ë’¤ í…ìŠ¤íŠ¸ê°€ ëª¨ë‘ ìˆëŠ” ê²½ìš°ë§Œ ë¶„ì„
            if before_text and after_text:
                before_sentiment = analyze_sentiment(before_text)
                after_sentiment = analyze_sentiment(after_text)
                
                result.update({
                    'has_context_change': True,
                    'transition_word': transition_word,
                    'before_transition': before_text,
                    'after_transition': after_text,
                    'before_sentiment': before_sentiment,
                    'after_sentiment': after_sentiment
                })
                
                # ë¬¸ë§¥ ë³€í™” ê°ì§€
                if before_sentiment == 'negative' and after_sentiment == 'positive':
                    result['positive_after_negative'] = True
                elif before_sentiment == 'positive' and after_sentiment == 'negative':
                    result['negative_after_positive'] = True
                
                break
    
    return result

def analyze_sentiment(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)"""
    if not text or len(text.strip()) < 2:
        return 'neutral'
    
    # ê¸ì •ì  í‚¤ì›Œë“œ íŒ¨í„´
    positive_patterns = [
        r'ì„±ê³µ|ì„±ê³¼|ê°œì„ |í–¥ìƒ|ì¦ê°€|ë‹¬ì„±|ì™„ë£Œ|í•´ê²°|ê·¹ë³µ|ë°œì „|ì„±ì¥|ë„ì•½|í˜ì‹ |ì°½ì˜|íš¨ìœ¨|ìµœì í™”',
        r'ì¢‹ì€|í›Œë¥­í•œ|ìš°ìˆ˜í•œ|ë›°ì–´ë‚œ|íƒì›”í•œ|ìµœê³ ì˜|ìµœìƒì˜|ì™„ë²½í•œ|ì™„ì „í•œ|ì™„ì„±ëœ',
        r'ë§Œì¡±|ê¸°ì¨|í¬ë§|ìì‹ ê°|ê¸ì •|ë‚™ê´€|ì—´ì •|ì˜ì§€|ë…¸ë ¥|ì„±ì‹¤|ì±…ì„ê°|ì£¼ë„ì„±',
        r'ë°°ì› ë‹¤|ì„±ì¥í–ˆë‹¤|ê°œì„ í–ˆë‹¤|í•´ê²°í–ˆë‹¤|ë‹¬ì„±í–ˆë‹¤|ì™„ë£Œí–ˆë‹¤|ê·¹ë³µí–ˆë‹¤|ë°œì „í–ˆë‹¤',
        r'ì˜\s*í–ˆë‹¤|ì„±ê³µí–ˆë‹¤|ì™„ë£Œí–ˆë‹¤|í•´ê²°í–ˆë‹¤|ê°œì„ í–ˆë‹¤|í–¥ìƒí–ˆë‹¤|ì¦ê°€í–ˆë‹¤|ë‹¬ì„±í–ˆë‹¤',
        r'ì¢‹ì•˜ë‹¤|í›Œë¥­í–ˆë‹¤|ìš°ìˆ˜í–ˆë‹¤|ë›°ì–´ë‚¬ë‹¤|íƒì›”í–ˆë‹¤|ì™„ë²½í–ˆë‹¤|ì™„ì „í–ˆë‹¤|ì™„ì„±í–ˆë‹¤'
    ]
    
    # ë¶€ì •ì  í‚¤ì›Œë“œ íŒ¨í„´
    negative_patterns = [
        r'ì‹¤íŒ¨|ì‹¤íŒ¨í–ˆë‹¤|ì‹¤íŒ¨í–ˆê³ |ì‹¤íŒ¨í–ˆìœ¼ë©°|ì‹¤íŒ¨í–ˆì§€ë§Œ|ì‹¤íŒ¨í–ˆê³ |ì‹¤íŒ¨í–ˆìœ¼ë‹ˆ|ì‹¤íŒ¨í–ˆìœ¼ë¯€ë¡œ',
        r'ì–´ë ¤ì›€|ì–´ë ¤ì› ë‹¤|ì–´ë ¤ì› ê³ |ì–´ë ¤ì› ìœ¼ë©°|ì–´ë ¤ì› ì§€ë§Œ|ì–´ë ¤ì› ê³ |ì–´ë ¤ì› ìœ¼ë‹ˆ|ì–´ë ¤ì› ìœ¼ë¯€ë¡œ',
        r'ë¬¸ì œ|ë¬¸ì œê°€|ë¬¸ì œë¥¼|ë¬¸ì œì—|ë¬¸ì œë¡œ|ë¬¸ì œì™€|ë¬¸ì œëŠ”|ë¬¸ì œë„|ë¬¸ì œë§Œ|ë¬¸ì œê¹Œì§€',
        r'ì‹¤ìˆ˜|ì‹¤ìˆ˜í–ˆë‹¤|ì‹¤ìˆ˜í–ˆê³ |ì‹¤ìˆ˜í–ˆìœ¼ë©°|ì‹¤ìˆ˜í–ˆì§€ë§Œ|ì‹¤ìˆ˜í–ˆê³ |ì‹¤ìˆ˜í–ˆìœ¼ë‹ˆ|ì‹¤ìˆ˜í–ˆìœ¼ë¯€ë¡œ',
        r'ë¶€ì¡±|ë¶€ì¡±í–ˆë‹¤|ë¶€ì¡±í–ˆê³ |ë¶€ì¡±í–ˆìœ¼ë©°|ë¶€ì¡±í–ˆì§€ë§Œ|ë¶€ì¡±í–ˆê³ |ë¶€ì¡±í–ˆìœ¼ë‹ˆ|ë¶€ì¡±í–ˆìœ¼ë¯€ë¡œ',
        r'ë¯¸í¡|ë¯¸í¡í–ˆë‹¤|ë¯¸í¡í–ˆê³ |ë¯¸í¡í–ˆìœ¼ë©°|ë¯¸í¡í–ˆì§€ë§Œ|ë¯¸í¡í–ˆê³ |ë¯¸í¡í–ˆìœ¼ë‹ˆ|ë¯¸í¡í–ˆìœ¼ë¯€ë¡œ',
        r'ë¶€ì¡±í•¨|ë¶€ì¡±í•¨ì„|ë¶€ì¡±í•¨ì—|ë¶€ì¡±í•¨ìœ¼ë¡œ|ë¶€ì¡±í•¨ê³¼|ë¶€ì¡±í•¨ì€|ë¶€ì¡±í•¨ë„|ë¶€ì¡±í•¨ë§Œ|ë¶€ì¡±í•¨ê¹Œì§€',
        r'ì‹¤ë§|ì‹¤ë§í–ˆë‹¤|ì‹¤ë§í–ˆê³ |ì‹¤ë§í–ˆìœ¼ë©°|ì‹¤ë§í–ˆì§€ë§Œ|ì‹¤ë§í–ˆê³ |ì‹¤ë§í–ˆìœ¼ë‹ˆ|ì‹¤ë§í–ˆìœ¼ë¯€ë¡œ',
        r'ì¢Œì ˆ|ì¢Œì ˆí–ˆë‹¤|ì¢Œì ˆí–ˆê³ |ì¢Œì ˆí–ˆìœ¼ë©°|ì¢Œì ˆí–ˆì§€ë§Œ|ì¢Œì ˆí–ˆê³ |ì¢Œì ˆí–ˆìœ¼ë‹ˆ|ì¢Œì ˆí–ˆìœ¼ë¯€ë¡œ',
        r'í˜ë“¤ì—ˆë‹¤|ì–´ë ¤ì› ë‹¤|ë§‰ë§‰í–ˆë‹¤|ë‹¹í™©í–ˆë‹¤|í˜¼ë€ìŠ¤ëŸ¬ì› ë‹¤|ë¶ˆì•ˆí–ˆë‹¤|ê±±ì •í–ˆë‹¤',
        r'ë‚˜ìœ|ì•ˆì¢‹ì€|ë¶€ì¡±í•œ|ë¯¸í¡í•œ|ì‹¤íŒ¨í•œ|ì‹¤íŒ¨í–ˆë‹¤|ì‹¤íŒ¨í–ˆê³ |ì‹¤íŒ¨í–ˆìœ¼ë©°|ì‹¤íŒ¨í–ˆì§€ë§Œ',
        r'ì–´ë ¤ì› ë‹¤|í˜ë“¤ì—ˆë‹¤|ë§‰ë§‰í–ˆë‹¤|ë‹¹í™©í–ˆë‹¤|í˜¼ë€ìŠ¤ëŸ¬ì› ë‹¤|ë¶ˆì•ˆí–ˆë‹¤|ê±±ì •í–ˆë‹¤|ì‹¤ë§í–ˆë‹¤|ì¢Œì ˆí–ˆë‹¤'
    ]
    
    # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ì¹´ìš´íŠ¸
    positive_count = 0
    negative_count = 0
    
    for pattern in positive_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            positive_count += 1
    
    for pattern in negative_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            negative_count += 1
    
    # ê°ì • íŒë‹¨ (í‚¤ì›Œë“œ ê°œìˆ˜ì™€ ê°€ì¤‘ì¹˜ ê³ ë ¤)
    if positive_count > negative_count and positive_count > 0:
        return 'positive'
    elif negative_count > positive_count and negative_count > 0:
        return 'negative'
    else:
        return 'neutral'

def has_positive_content_after_transition(sentence: str) -> bool:
    """ì „í™˜ì–´ ë’¤ì— ê¸ì •ì  ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸ (ê¸°ì¡´ í•¨ìˆ˜ - í˜¸í™˜ì„± ìœ ì§€)"""
    context_analysis = analyze_transition_context(sentence)
    return context_analysis.get('positive_after_negative', False)

def has_negative_content_after_transition(sentence: str) -> bool:
    """ì „í™˜ì–´ ë’¤ì— ë¶€ì •ì  ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸ (ê¸°ì¡´ í•¨ìˆ˜ - í˜¸í™˜ì„± ìœ ì§€)"""
    context_analysis = analyze_transition_context(sentence)
    return context_analysis.get('negative_after_positive', False)

def analyze_resume_content(state: Dict[str, Any]) -> Dict[str, Any]:
    """ì´ë ¥ì„œ ë‚´ìš© ë¶„ì„ ë…¸ë“œ"""
    resume_content = state.get("resume_content", "")
    jobpost_id = state.get("jobpost_id")
    company_id = state.get("company_id")
    
    # ì´ë ¥ì„œ ë‚´ìš©ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë¶„ì„
    analysis_result = {
        "content_length": len(resume_content),
        "has_education": "í•™ë ¥" in resume_content or "êµìœ¡" in resume_content,
        "has_experience": "ê²½ë ¥" in resume_content or "ê²½í—˜" in resume_content,
        "has_skills": "ê¸°ìˆ " in resume_content or "ìŠ¤í‚¬" in resume_content,
        "has_projects": "í”„ë¡œì íŠ¸" in resume_content or "í™œë™" in resume_content,
        "content_sections": []
    }
    
    # ì„¹ì…˜ë³„ë¡œ ë¶„ë¦¬
    sections = re.split(r'\n\s*\n', resume_content)
    for section in sections:
        if section.strip():
            analysis_result["content_sections"].append(section.strip())
    
    return {
        **state,
        "content_analysis": analysis_result,
        "next": "generate_highlight_criteria"
    }

def generate_highlight_criteria(state: Dict[str, Any]) -> Dict[str, Any]:
    """í•˜ì´ë¼ì´íŒ… ê¸°ì¤€ ìƒì„± ë…¸ë“œ"""
    content_analysis = state.get("content_analysis", {})
    resume_content = state.get("resume_content", "")
    jobpost_id = state.get("jobpost_id")
    company_id = state.get("company_id")
    
    # ê¸°ë³¸ í•˜ì´ë¼ì´íŒ… ê¸°ì¤€ ì •ì˜ (ë³´ë¼ì— ì¶”ìƒ í¬í•¨)
    highlight_criteria = {
        "red": {
            "name": "ì§ë¬´ ë¶ˆì¼ì¹˜ (Mismatch)",
            "description": "ì§ë¬´ ë„ë©”ì¸/ì—­í•  ë¶ˆì¼ì¹˜í•˜ëŠ” êµ¬ì ˆ, ìê²©ìš”ê±´ ìŠ¤íƒ 'í•™ìŠµ/ì˜ˆì •' ìˆ˜ì¤€ì¸ êµ¬ì ˆ"
        },
        "orange": {
            "name": "ë¶€ì • íƒœë„ (Negative Tone)",
            "description": "ì±…ì„íšŒí”¼Â·ê³µê²©/ë¹„ë‚œÂ·ë¹„ìœ¤ë¦¬Â·í—ˆìœ„/ê³¼ì¥ ì˜ì‹¬Â·ì†Œí†µê²°ì—¬ ë“±ì˜ ë¶€ì •ì íƒœë„ ë¦¬ìŠ¤í¬"
        },
        "yellow": {
            "name": "ì¸ì¬ìƒ ê°€ì¹˜ (Value Fit)",
            "description": "íšŒì‚¬ ì¸ì¬ìƒê³¼  ë§ëŠ” í–‰ë™Â·ì‚¬ë¡€ë¡œ ì¶”ì •ë˜ëŠ” êµ¬ì ˆ(ì ìˆ˜í™” X)"
        },
        "blue": {
            "name": "ê¸°ìˆ  ì‚¬ìš© ê²½í—˜ (Tech Evidence)",
            "description": "ë„êµ¬/ì–¸ì–´/í”„ë ˆì„ì›Œí¬ë¥¼ ì‹¤ì œë¡œ ì‚¬ìš©í•œ ê·¼ê±°ê°€ ë“œëŸ¬ë‚˜ëŠ” êµ¬ì ˆ"
        },
        "purple": {
            "name": "ê²½í—˜Â·ì„±ê³¼Â·ì´ë ¥Â·ê²½ë ¥ (Experience/Impact)",
            "description": "í”„ë¡œì íŠ¸Â·êµìœ¡Â·ê²½ë ¥Â·ìˆ˜ìƒ ë“± ê²°ê³¼Â·ì„íŒ©íŠ¸ **ë°** ì¶”ìƒí‘œí˜„(ë©´ì ‘ í™•ì¸ìš©)ì„ í•¨ê»˜ í¬í•¨"
        }
    }
    
    return {
        **state,
        "highlight_criteria": highlight_criteria,
        "next": "perform_advanced_highlighting"
    }

def get_yellow_prompt(values_keywords: List[str], candidates: List[Dict[str, Any]], full_text: str) -> str:
    """ë…¸ë€ìƒ‰ í•˜ì´ë¼ì´íŠ¸ìš© í”„ë¡¬í”„íŠ¸ (íšŒì‚¬ ì¸ì¬ìƒ ë§¤ì¹­)"""
    value_keywords_comma = ', '.join(values_keywords)
    sentences_json = json.dumps([c['sentence'] for c in candidates], ensure_ascii=False, indent=2)

    return f"""
    ### ì—­í• 
    ë‹¹ì‹ ì€ ìê¸°ì†Œê°œì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìê¸°ì†Œê°œì„œì—ì„œ íšŒì‚¬ ì¸ì¬ìƒ ê°€ì¹˜ê°€ ì‹¤ì œ í–‰ë™/ì‚¬ë¡€ë¡œ êµ¬í˜„ëœ êµ¬ì ˆì„ ë¼ë²¨ë§í•˜ì„¸ìš”.

    ### ë¶„ì„ ê¸°ì¤€ í‚¤ì›Œë“œ
    {value_keywords_comma}



    ### ë¶„ì„í•  ë¬¸ì¥ë“¤
    {sentences_json}

    ### ë§¤ì¹­ ìœ í˜• ë° ê¸°ì¤€
    **[1] ì •í™•í•œ ë§¤ì¹­ (exact)**
    - í‚¤ì›Œë“œê°€ ê·¸ëŒ€ë¡œ ì–¸ê¸‰ëœ ê²½ìš°
    - ì˜ˆ: "ê³µìµ","íˆ¬ëª…ì„±","ì „ë¬¸ì„±"

    **[2] ë¬¸ë§¥ì  ë§¤ì¹­ (semantic)**
    - í‚¤ì›Œë“œì™€ ê°™ì€ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„
    - ìœ ì‚¬ì–´/ë™ì˜ì–´/ê´€ë ¨ ê°œë…ìœ¼ë¡œ í‘œí˜„ëœ ê²½ìš°
    - ì‹¤ì œ í–‰ë™ì´ë‚˜ ì‚¬ë¡€ë¡œ ê°€ì¹˜ê°€ ë“œëŸ¬ë‚˜ëŠ” ê²½ìš°

    **[3] ë¬¸ë§¥ì  ë§¤ì¹­ ì˜ˆì‹œ**
    - "ê³µìµ" ê´€ë ¨: ì‚¬íšŒ ê¸°ì—¬, ë´‰ì‚¬, ë‚˜ëˆ”, ì§€ì—­ ë°œì „, í™˜ê²½ ë³´í˜¸, ì‚¬íšŒì  ê°€ì¹˜, ê³µê³µ ì´ìµ
    - "ì±…ì„" ê´€ë ¨: ì£¼ë„ì , ì±…ì„ì§€ê³ , ì™„ìˆ˜, ì„±ê³¼ ë‹¬ì„±, ì‹ ë¢°, ì˜ë¬´ê°, ì„±ì‹¤í•¨, ê¼¼ê¼¼í•¨
    - "í˜ì‹ " ê´€ë ¨: ê°œì„ , ìµœì í™”, íš¨ìœ¨í™”, ìƒˆë¡œìš´ ë°©ë²•, ì°½ì˜ì  í•´ê²°, ë³€í™”, ë„ì „, í˜ì‹ ì  ì‚¬ê³ 
    - "ì†Œí†µ" ê´€ë ¨: ì˜ê²¬ êµí™˜, ëŒ€í™”, ì„¤ëª…, ì „ë‹¬, ì´í•´, ê³µê°, ëª…í™•í•œ ì „ë‹¬, í”¼ë“œë°±
    - "í˜‘ì—…" ê´€ë ¨: íŒ€ì›Œí¬, ì¡°ìœ¨, ê³µë™ ì‘ì—…, ë‹¤ë¶€ì„œ í˜‘ë ¥, ìƒí˜¸ ë³´ì™„, ì‹œë„ˆì§€, í˜‘ë ¥ì  ë¬¸ì œ í•´ê²°

    ### ë¼ë²¨ë§ ê·œì¹™
    - **ë¬¸ë§¥ì  ë§¤ì¹­ì„ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”** (ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ë³´ë‹¤ ì˜ë¯¸ì  ì—°ê²°ì´ ì¤‘ìš”)
    - ìŠ¬ë¡œê±´Â·ë‹¤ì§ë¥˜(ì˜ˆ: "í˜ì‹ ê³¼ í˜‘ì—…ì„ ì¤‘ì‹œí•©ë‹ˆë‹¤", "ìµœê³ ê°€ ë˜ê² ìŠµë‹ˆë‹¤") ë° ê·¼ê±° ì—†ëŠ” ë‚˜ì—´ ë¬¸ì¥ ì œì™¸ 
    - ê° valueë‹¹ ìµœëŒ€ 2ê°œë§Œ ì„ íƒí•´ ì‘ë‹µí•˜ì„¸ìš”. ìœ ì‚¬í•˜ê±°ë‚˜ ì•½í•œ ê±´ ì œì™¸
    - ê°€ëŠ¥í•œ í•œ ì˜ë¯¸ ìˆëŠ” **ìµœì†Œ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì ˆ**ë§Œ ì¶”ì¶œ (ë¬¸ì¥ ì „ì²´ê°€ ì•„ë‹Œ êµ¬ì ˆ ë‹¨ìœ„)
    - **ì‘ë‹µì€ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”** (ë¶„ì„ ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´)

    ### JSON ì‘ë‹µ í¬ë§·
    {{
        "highlights": [
            {{
                "sentence": "ì¶”ì¶œëœ êµ¬ì ˆ",
                "category": "value_fit",
                "reason": "ì„±ëŠ¥ ê°œì„ ì„ í†µí•œ í˜ì‹ ì  í•´ê²°ì±… ì œì‹œ"
            }},
            â€¦
        ]
    }}
    """

def get_blue_prompt(skill_keywords: List[str], candidates: List[Dict[str, Any]], full_text: str) -> str:
    """íŒŒë€ìƒ‰ í•˜ì´ë¼ì´íŠ¸ìš© í”„ë¡¬í”„íŠ¸ (ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„)"""
    skills_comma = ', '.join(skill_keywords)
    sentences_json = json.dumps([c['sentence'] for c in candidates], ensure_ascii=False, indent=2)

    return f"""
    ### ì—­í• 
    ë‹¹ì‹ ì€ ìê¸°ì†Œê°œì„œì—ì„œ ì‹¤ì œ ê¸°ìˆ  ì‚¬ìš© ê²½í—˜ì´ ë“œëŸ¬ë‚˜ëŠ” ë¶€ë¶„ì„ ì°¾ì•„ë‚´ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

    ### ë¶„ì„ ê¸°ì¤€ ê¸°ìˆ  í‚¤ì›Œë“œ
    {skills_comma}

    í•œê¸€ë¡œ í‘œê¸°ëœ ê¸°ìˆ  í‚¤ì›Œë“œ(ì˜ˆ: ìë°”, ìŠ¤í”„ë§ ë“±)ë„ ë™ì¼í•˜ê²Œ ê¸°ìˆ ë¡œ ì¸ì‹í•˜ì—¬ íŒë‹¨í•˜ì„¸ìš”.

    ### ë¶„ì„í•  ë¬¸ì¥ë“¤
    {sentences_json}

    ### ë§¤ì¹­ ê¸°ì¤€ (ìœ ì‚¬ í‚¤ì›Œë“œ ë§¤ì¹­)
    **[1] ê¸°ìˆ  í‚¤ì›Œë“œ ë§¤ì¹­ (ëŒ€/ì†Œë¬¸ì, í•œ/ì˜ êµ¬ë¶„ ì—†ìŒ)**
    - ì±„ìš©ê³µê³ ì— ëª…ì‹œëœ ê¸°ìˆ  í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ í‘œí˜„ ëª¨ë‘ ë§¤ì¹­
    - ëŒ€/ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ: "Java" = "java" = "JAVA" = "Java"
    - í•œ/ì˜ êµ¬ë¶„ ì—†ìŒ: "ìë°”" = "Java" = "JAVA", "ìŠ¤í”„ë§" = "Spring" = "SPRING"
    - ì˜ˆ: ì±„ìš©ê³µê³ ì— "Java"ê°€ ìˆìœ¼ë©´ "Java", "java", "JAVA", "ìë°”" ëª¨ë‘ ë§¤ì¹­

    **[2] ìœ ì‚¬ í‚¤ì›Œë“œ ë§¤ì¹­ ì˜ˆì‹œ**
    - Java: "Java", "java", "JAVA", "ìë°”"
    - Spring: "Spring", "spring", "SPRING", "ìŠ¤í”„ë§"
    - AWS: "AWS", "aws", "Aws", "ì•„ë§ˆì¡´", "Amazon"
    - React: "React", "react", "REACT", "ë¦¬ì•¡íŠ¸"
    - Python: "Python", "python", "PYTHON", "íŒŒì´ì¬"

    **[3] ì„ íƒ ê¸°ì¤€**
    1. ë¬¸ì¥ ì•ˆì— ì±„ìš©ê³µê³ ì˜ ê¸°ìˆ  í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ í‘œí˜„ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    2. í•´ë‹¹ í‚¤ì›Œë“œê°€ ì‹¤ì œë¡œ ì‚¬ìš©ëœ ê²½í—˜, ê¸°ì—¬, í™œë™ì„ ì˜ë¯¸í•´ì•¼ í•©ë‹ˆë‹¤.
       - ì˜ˆ: ì‚¬ìš©í–ˆë‹¤, ì ìš©í–ˆë‹¤, í”„ë¡œì íŠ¸ì— í™œìš©í–ˆë‹¤, ê°œë°œí–ˆë‹¤, ì„¤ì •í–ˆë‹¤ ë“±
    3. ë‹¨ìˆœ ì–¸ê¸‰, í•™ìŠµ ì˜ˆì •, í¥ë¯¸ í‘œí˜„, ëª©í‘œë§Œ ë‹´ê¸´ ë¶€ë¶„ì€ ì œì™¸í•©ë‹ˆë‹¤.
       - ì˜ˆ: ë°°ìš°ê³  ì‹¶ë‹¤, ê³µë¶€ ì¤‘ì´ë‹¤, ì¤€ë¹„í•˜ê³  ìˆë‹¤, í¥ë¯¸ê°€ ìˆë‹¤ ë“±
    4. **ì¤‘ìš”**: ê¸°ìˆ  í‚¤ì›Œë“œì˜ ìœ ì‚¬ í‘œí˜„ë§Œ ë§¤ì¹­í•˜ì„¸ìš”. ë¬¸ë§¥ì  ë§¤ì¹­ì€ í•˜ì§€ ë§ˆì„¸ìš”.
       - ì˜ˆ: "ë°±ì—”ë“œ ê°œë°œ"ì´ ì±„ìš©ê³µê³ ì— ì—†ìœ¼ë©´ ë§¤ì¹­ ì•ˆë¨
       - ì˜ˆ: "ì›¹ ê°œë°œ"ì´ ì±„ìš©ê³µê³ ì— ì—†ìœ¼ë©´ ë§¤ì¹­ ì•ˆë¨

    ### ë¼ë²¨ë§ ê·œì¹™
    - ê°€ëŠ¥í•œ í•œ ì˜ë¯¸ ìˆëŠ” **ìµœì†Œ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì ˆ**ë§Œ ì¶”ì¶œ (ë¬¸ì¥ ì „ì²´ê°€ ì•„ë‹Œ êµ¬ì ˆ ë‹¨ìœ„)
    - ê¸°ìˆ  í‚¤ì›Œë“œê°€ ì‹¤ì œ í–‰ë™ê³¼ ì—°ê²°ë˜ì–´ì•¼ í•¨
    - ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì€ í•˜ë‚˜ë§Œ ì¶”ì¶œ

    ### JSON ì‘ë‹µ í¬ë§·
    {{
        "highlights": [
            {{
                "sentence": "ê¸°ìˆ  ì‚¬ìš© ê²½í—˜ì´ ë“œëŸ¬ë‚˜ëŠ” êµ¬ì ˆ",
                "category": "skill_fit",
                "reason": "ê¸°ìˆ ì´ ì‹¤ì œë¡œ í™œìš©ëœ ê²½í—˜"
            }}
        ]
    }}
    """

def get_red_prompt(mismatch_keywords: List[str], candidates: List[Dict[str, Any]], full_text: str, job_details: str = "") -> str:
    """ë¹¨ê°„ìƒ‰ í•˜ì´ë¼ì´íŠ¸ìš© í”„ë¡¬í”„íŠ¸ (ì§ë¬´ ë¶ˆì¼ì¹˜)"""
    sentences_json = json.dumps([c['sentence'] for c in candidates], ensure_ascii=False, indent=2)
    
    return f"""
    ### ì—­í• 
    ë‹¹ì‹ ì€ ìê¸°ì†Œê°œì„œì—ì„œ ì§ë¬´ ë„ë©”ì¸/ì—­í•  ë¶ˆì¼ì¹˜ ìš”ì†Œë¥¼ ì°¾ì•„ë‚´ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    ### ë¶„ì„ ê¸°ì¤€ ë¶ˆì¼ì¹˜ í‚¤ì›Œë“œ
    {', '.join(mismatch_keywords)}

    ### ë¶„ì„í•  ë¬¸ì¥ë“¤
    {sentences_json}

    ### ì§ë¬´ ë¶ˆì¼ì¹˜ ìœ í˜•
    **[1] ì§ë¬´ ë„ë©”ì¸ ë¶ˆì¼ì¹˜**
    - ì§€ì› ì§ë¬´ì™€ ì™„ì „íˆ ë‹¤ë¥¸ ë¶„ì•¼ì˜ ê²½í—˜
    - ì§€ì› ì§ë¬´ì™€ ê´€ë ¨ ì—†ëŠ” ì—…ë¬´ ê²½í—˜
    - ì§€ì› ì§ë¬´ì™€ ë‹¤ë¥¸ ì‚°ì—… ë¶„ì•¼ì˜ ê²½í—˜

    **[2] ì—­í•  ë¶ˆì¼ì¹˜**
    - ì§€ì› ì§ë¬´ì™€ ë‹¤ë¥¸ ì—­í• ì˜ ê²½í—˜
    - ì§€ì› ì§ë¬´ë³´ë‹¤ ë‚®ì€ ìˆ˜ì¤€ì˜ ì—­í•  ê²½í—˜
    - ì§€ì› ì§ë¬´ì™€ ë§ì§€ ì•ŠëŠ” ë¦¬ë”ì‹­ ê²½í—˜

    **[3] ìê²©ìš”ê±´ ìŠ¤íƒ 'í•™ìŠµ/ì˜ˆì •' ìˆ˜ì¤€**
    - "ë°°ìš°ê³  ìˆë‹¤", "í•™ìŠµ ì¤‘ì´ë‹¤", "ì¤€ë¹„ ì¤‘ì´ë‹¤" ë“±
    - "~í•  ì˜ˆì •ì´ë‹¤", "~í•˜ë ¤ê³  í•œë‹¤" ë“± ë¯¸ë˜í˜• í‘œí˜„
    - ì‹¤ì œ ì‚¬ìš© ê²½í—˜ì´ ì•„ë‹Œ í•™ìŠµ ì˜ë„ë§Œ í‘œí˜„

    ### ë¼ë²¨ë§ ê·œì¹™
    - ì§ë¬´ì™€ ì§ì ‘ì ìœ¼ë¡œ ë¶ˆì¼ì¹˜í•˜ëŠ” ìš”ì†Œë¥¼ ì°¾ìœ¼ì„¸ìš”
    - ìê²©ìš”ê±´ì— ë¯¸ë‹¬í•˜ëŠ” ê¸°ìˆ  ìˆ˜ì¤€ì„ ì°¾ìœ¼ì„¸ìš”
    - êµ¬ì²´ì ì´ê³  ê°ê´€ì ì¸ ë¶ˆì¼ì¹˜ ìš”ì†Œë¥¼ ì„ íƒí•˜ì„¸ìš”

    ### JSON ì‘ë‹µ í¬ë§·
    {{
        "highlights": [
            {{
                "sentence": "ì§ë¬´ ë¶ˆì¼ì¹˜ê°€ ë“œëŸ¬ë‚˜ëŠ” êµ¬ì ˆ",
                "category": "mismatch",
                "reason": "ë¶ˆì¼ì¹˜ ìš”ì†Œ ì„¤ëª…"
            }}
        ]
    }}
    """

def get_orange_prompt(negative_keywords: List[str], candidates: List[Dict[str, Any]], full_text: str) -> str:
    """ì˜¤ë Œì§€ìƒ‰ í•˜ì´ë¼ì´íŠ¸ìš© í”„ë¡¬í”„íŠ¸ (ë¶€ì • íƒœë„)"""
    sentences_json = json.dumps([c['sentence'] for c in candidates], ensure_ascii=False, indent=2)
    
    return f"""
    ### ì—­í• 
    ë‹¹ì‹ ì€ ìê¸°ì†Œê°œì„œì—ì„œ ë¶€ì •ì  íƒœë„ë‚˜ ìœ¤ë¦¬ì  ë¬¸ì œë¥¼ ì°¾ì•„ë‚´ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    ### ë¶„ì„ ê¸°ì¤€ ë¶€ì • í‚¤ì›Œë“œ
    {', '.join(negative_keywords)}

    ### ë¶„ì„í•  ë¬¸ì¥ë“¤
    {sentences_json}

    ### ë¶€ì • íƒœë„ ìœ í˜•
    **[1] ì±…ì„íšŒí”¼**
    - ì‹¤íŒ¨ë‚˜ ë¬¸ì œì— ëŒ€í•œ ì±…ì„ì„ íšŒí”¼í•˜ëŠ” í‘œí˜„
    - "~ë•Œë¬¸ì—", "~íƒ“ì—" ë“± ì™¸ë¶€ ìš”ì¸ íƒ“ìœ¼ë¡œ ëŒë¦¬ëŠ” í‘œí˜„
    - ê°œì¸ì  ì±…ì„ì„ ì¸ì •í•˜ì§€ ì•ŠëŠ” íƒœë„

    **[2] ê³µê²©/ë¹„ë‚œ**
    - ë‹¤ë¥¸ ì‚¬ëŒì´ë‚˜ ì¡°ì§ì„ ë¹„ë‚œí•˜ëŠ” í‘œí˜„
    - ê³¼ë„í•˜ê²Œ ë¶€ì •ì ì¸ ì‹œê°ìœ¼ë¡œ ë°”ë¼ë³´ëŠ” íƒœë„
    - ê±´ì„¤ì ì´ì§€ ì•Šì€ ë¹„íŒì  í‘œí˜„

    **[3] ë¹„ìœ¤ë¦¬ì  í‘œí˜„**
    - ìœ¤ë¦¬ì ìœ¼ë¡œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” í‘œí˜„
    - ë¶€ì •ì§í•˜ê±°ë‚˜ ì†ì„ìˆ˜ë¥¼ ì•”ì‹œí•˜ëŠ” í‘œí˜„
    - ë„ë•ì ìœ¼ë¡œ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í–‰ë™ì´ë‚˜ íƒœë„

    **[4] í—ˆìœ„/ê³¼ì¥ ì˜ì‹¬**
    - ì‚¬ì‹¤ê³¼ ë‹¤ë¥¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê³¼ì¥ëœ í‘œí˜„
    - ê²€ì¦í•˜ê¸° ì–´ë ¤ìš´ ê³¼ë„í•œ ì„±ê³¼ë‚˜ ê²½í—˜
    - ì‹ ë¢°ì„±ì´ ì˜ì‹¬ë˜ëŠ” êµ¬ì²´ì  ìˆ˜ì¹˜ë‚˜ ê²°ê³¼
    - "ìµœê³ ì˜", "ì™„ë²½í•œ", "íƒì›”í•œ" ë“± ê³¼ë„í•œ ìˆ˜ì‹ì–´ ì‚¬ìš©
    - "ë§¤ìš°", "ì •ë§", "ë„ˆë¬´", "ì—„ì²­" ë“± ê³¼ì¥ëœ ë¶€ì‚¬ ì‚¬ìš©
    - "100%", "ì™„ë²½", "ì ˆëŒ€" ë“± ê·¹ë‹¨ì  í‘œí˜„
    - êµ¬ì²´ì  ê·¼ê±° ì—†ì´ "ì••ë„ì ", "ìµœê³ " ë“± ì£¼ì¥

    **[5] ì†Œí†µê²°ì—¬**
    - í˜‘ì—…ì´ë‚˜ ì†Œí†µì— ë¶€ì •ì ì¸ íƒœë„
    - ê°œì¸ì£¼ì˜ì ì´ê±°ë‚˜ íŒ€ì›Œí¬ë¥¼ ë¬´ì‹œí•˜ëŠ” í‘œí˜„
    - ì˜ì‚¬ì†Œí†µ ëŠ¥ë ¥ ë¶€ì¡±ì„ ë³´ì—¬ì£¼ëŠ” í‘œí˜„

    ### ë¼ë²¨ë§ ê·œì¹™
    - ë¶€ì •ì  íƒœë„ë‚˜ ìœ¤ë¦¬ì  ë¬¸ì œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¾ìœ¼ì„¸ìš”
    - êµ¬ì²´ì ì´ê³  ê°ê´€ì ì¸ ë¶€ì • ìš”ì†Œë¥¼ ì„ íƒí•˜ì„¸ìš”
    - ë‹¨ìˆœí•œ ë¶€ì¡±í•¨ë³´ë‹¤ëŠ” íƒœë„ë‚˜ ìœ¤ë¦¬ì  ë¬¸ì œë¥¼ ì°¾ìœ¼ì„¸ìš”
    - **ì¤‘ìš”**: ë¶€ì •ì  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ì´ë¼ë„ ë¬¸ë§¥ìƒ ê¸ì •ì ì´ë©´ ì œì™¸í•˜ì„¸ìš”
    - **ì¤‘ìš”**: ì „í™˜ì–´(í•˜ì§€ë§Œ, ê·¸ëŸ¬ë‚˜, ê·¸ëŸ°ë° ë“±) ì• ë˜ëŠ” ë’¤ì— ê¸ì •ì  ë‚´ìš©ì´ ìˆìœ¼ë©´ ì œì™¸í•˜ì„¸ìš”
    - **ì¤‘ìš”**: ì‹¤ì œë¡œ ë¶€ì •ì  íƒœë„ë‚˜ ìœ¤ë¦¬ì  ë¬¸ì œê°€ ë“œëŸ¬ë‚˜ëŠ” ë¬¸ì¥ë§Œ ì„ íƒí•˜ì„¸ìš”

    ### JSON ì‘ë‹µ í¬ë§·
    {{
        "highlights": [
            {{
                "sentence": "ë¶€ì • íƒœë„ê°€ ë“œëŸ¬ë‚˜ëŠ” êµ¬ì ˆ",
                "category": "negative_tone",
                "reason": "ë¶€ì • íƒœë„ ì„¤ëª…"
            }}
        ]
    }}
    """

def get_purple_prompt(experience_keywords: List[str], candidates: List[Dict[str, Any]], full_text: str) -> str:
    """ë³´ë¼ìƒ‰ í•˜ì´ë¼ì´íŠ¸ìš© í”„ë¡¬í”„íŠ¸ (ê²½í—˜Â·ì„±ê³¼Â·ì´ë ¥Â·ê²½ë ¥ + ì¶”ìƒí‘œí˜„)"""
    sentences_json = json.dumps([c['sentence'] for c in candidates], ensure_ascii=False, indent=2)
    
    return f"""
    ### ì—­í• 
    ë‹¹ì‹ ì€ ìê¸°ì†Œê°œì„œì—ì„œ ê²½í—˜Â·ì„±ê³¼Â·ì´ë ¥Â·ê²½ë ¥ê³¼ ì¶”ìƒí‘œí˜„ì„ í•¨ê»˜ ì°¾ì•„ë‚´ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    ### ë¶„ì„í•  ë¬¸ì¥ë“¤
    {sentences_json}

    ### ê²½í—˜Â·ì„±ê³¼Â·ì´ë ¥Â·ê²½ë ¥ ìœ í˜•
    **[1] êµ¬ì²´ì  ì„±ê³¼ê°€ ìˆëŠ” ê²½í—˜**
    - ìˆ˜ì¹˜í™”ëœ ì„±ê³¼ (ì˜ˆ: "ë§¤ì¶œ 20% ì¦ê°€", "ì‹œê°„ 30% ë‹¨ì¶•")
    - êµ¬ì²´ì  ê²°ê³¼ê°€ ìˆëŠ” í”„ë¡œì íŠ¸ë‚˜ í™œë™
    - ì‹¤ì œ ì„íŒ©íŠ¸ê°€ ë“œëŸ¬ë‚˜ëŠ” ê²½í—˜

    **[2] ë¬¸ì œ í•´ê²° ê²½í—˜**
    - ì‹¤ì œ ë¬¸ì œë¥¼ í•´ê²°í•œ ê²½í—˜
    - ì–´ë ¤ì›€ì„ ê·¹ë³µí•œ êµ¬ì²´ì  ì‚¬ë¡€
    - ë„ì „ì  ìƒí™©ì—ì„œì˜ ì„±ê³¼

    **[3] ë¦¬ë”ì‹­ ê²½í—˜**
    - íŒ€ì„ ì´ëˆ ê²½í—˜
    - ì£¼ë„ì ìœ¼ë¡œ ì§„í–‰í•œ í”„ë¡œì íŠ¸
    - ê´€ë¦¬Â·ì¡°ìœ¨ ê²½í—˜

    **[4] í•™ìŠµ ë° ì„±ì¥ ê²½í—˜**
    - ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ ì§€ì‹ì„ ìŠµë“í•œ ê²½í—˜
    - ì‹¤íŒ¨ë¥¼ í†µí•´ ë°°ìš´ êµ¬ì²´ì  êµí›ˆ
    - ì „ë¬¸ì„± í–¥ìƒ ê²½í—˜

    **[5] êµìœ¡Â·ìˆ˜ìƒÂ·ìê²© ê²½í—˜**
    - ê´€ë ¨ êµìœ¡ ì´ìˆ˜ ê²½í—˜
    - ìˆ˜ìƒ ê²½ë ¥
    - ìê²©ì¦ ì·¨ë“

    ### ì¶”ìƒí‘œí˜„ (ë©´ì ‘ í™•ì¸ìš©)
    **[6] êµ¬ì²´ì„± ë¶€ì¡±í•œ í‘œí˜„**
    - "~í•  ì˜ˆì •ì´ë‹¤", "~í•˜ë ¤ê³  í•œë‹¤" ë“± ë¯¸ë˜í˜• í‘œí˜„
    - êµ¬ì²´ì  ê³„íšì´ë‚˜ ì‹¤í˜„ ê°€ëŠ¥ì„±ì´ ë¶ˆë¶„ëª…í•œ ê²½ìš°
    - êµ¬ì²´ì  ì„±ê³¼ë‚˜ ê²°ê³¼ê°€ ì—†ëŠ” ì¶”ìƒì  í‘œí˜„

    **[7] ê²€ì¦ í•„ìš” í‘œí˜„**
    - "ì—´ì‹¬íˆ", "ìµœì„ ì„ ë‹¤í•´", "ì„±ì‹¤í•˜ê²Œ" ë“± êµ¬ì²´ì  ê·¼ê±° ì—†ëŠ” í‘œí˜„
    - ê³¼ë„í•œ ìì‹ ê° í‘œí˜„ ("ìµœê³ ", "ìµœì„ ", "ì™„ë²½")
    - ì£¼ê´€ì  í‰ê°€ ("ì¢‹ì€", "ë‚˜ìœ", "í›Œë¥­í•œ")

    **[8] ì¶”ê°€ ì§ˆë¬¸ ìœ ë°œ í‘œí˜„**
    - êµ¬ì²´ì  ìˆ˜ì¹˜ë‚˜ ê²°ê³¼ê°€ ì—†ëŠ” ì„±ê³¼ í‘œí˜„
    - ê¸°ìˆ ì´ë‚˜ ê²½í—˜ì˜ ì‹¤ì œ í™œìš© ì •ë„ê°€ ë¶ˆë¶„ëª…í•œ ê²½ìš°
    - íŒ€ì›Œí¬ë‚˜ í˜‘ì—…ì—ì„œì˜ êµ¬ì²´ì  ì—­í• ì´ ë¶ˆë¶„ëª…í•œ ê²½ìš°

    ### ë¼ë²¨ë§ ê·œì¹™
    - êµ¬ì²´ì ì´ê³  ì˜ë¯¸ ìˆëŠ” ê²½í—˜ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¾ìœ¼ì„¸ìš”
    - ì¶”ìƒí‘œí˜„ì€ ë©´ì ‘ì—ì„œ ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ë¶€ë¶„ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”
    - ê²½í—˜ê³¼ ì¶”ìƒí‘œí˜„ì„ ëª¨ë‘ í¬í•¨í•˜ì—¬ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”

    ### JSON ì‘ë‹µ í¬ë§·
    {{
        "highlights": [
            {{
                "sentence": "ê²½í—˜ì´ë‚˜ ì¶”ìƒí‘œí˜„ì´ ë“œëŸ¬ë‚˜ëŠ” êµ¬ì ˆ",
                "category": "experience",
                "reason": "ê²½í—˜ì˜ ì˜ë¯¸ ë˜ëŠ” ì¶”ìƒí‘œí˜„ í™•ì¸ í•„ìš”"
            }}
        ]
    }}
    """

async def analyze_category_with_llm(
    resume_content: str, 
    category: str, 
    keywords: List[str], 
    job_details: str = ""
) -> List[Dict[str, Any]]:
    """LLMì„ ì‚¬ìš©í•œ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„"""
    try:
        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[.!?]\s+', resume_content)
        candidates = [{"sentence": s.strip()} for s in sentences if s.strip()]
        
        if not candidates:
            return []
        
        # ì¹´í…Œê³ ë¦¬ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        if category == "yellow" or category == "value_fit":
            prompt = get_yellow_prompt(keywords, candidates, resume_content)
        elif category == "blue" or category == "skill_fit":
            prompt = get_blue_prompt(keywords, candidates, resume_content)
        elif category == "red" or category == "mismatch":
            prompt = get_red_prompt(keywords, candidates, resume_content, job_details)
        elif category == "orange" or category == "negative_tone":
            # ì˜¤ë Œì§€ìƒ‰ì€ ê°ì • ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ê»˜ ì‚¬ìš©
            return await analyze_orange_with_sentiment(candidates, resume_content)
        elif category == "purple" or category == "experience":
            prompt = get_purple_prompt(keywords, candidates, resume_content)
        else:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""
            ë‹¤ìŒ ìê¸°ì†Œê°œì„œì—ì„œ {category}ì™€ ê´€ë ¨ëœ ë¬¸ì¥ë“¤ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
            
            ë¶„ì„í•  ë¬¸ì¥ë“¤:
            {json.dumps([c['sentence'] for c in candidates], ensure_ascii=False, indent=2)}
            
            JSON ì‘ë‹µ í¬ë§·:
            {{
                "highlights": [
                    {{
                        "sentence": "ê´€ë ¨ êµ¬ì ˆ",
                        "category": "{category}",
                        "reason": "ì„ íƒ ì´ìœ "
                    }}
                ]
            }}
            """
        
        # LLM í˜¸ì¶œ
        response = await llm.ainvoke(prompt)
        
        # ì‘ë‹µ íŒŒì‹±
        try:
            result = json.loads(response.content)
            return result.get("highlights", [])
        except json.JSONDecodeError:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {response.content}")
            return []
            
    except Exception as e:
        print(f"LLM ë¶„ì„ ì˜¤ë¥˜ ({category}): {str(e)}")
        return []

async def analyze_orange_with_sentiment(candidates: List[Dict[str, Any]], full_text: str) -> List[Dict[str, Any]]:
    """ì˜¤ë Œì§€ìƒ‰ í•˜ì´ë¼ì´íŒ… - ê°ì • ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ ê²°í•©"""
    try:
        # ê°ì • ëª¨ë¸ ë¡œë“œ ì‹œë„
        sentiment_model = None
        sentiment_tokenizer = None
        
        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            import torch
            
            model_name = "nlp04/korean_sentiment_analysis_kcelectra"
            sentiment_tokenizer = AutoTokenizer.from_pretrained(model_name)
            sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_name)
            print("âœ… ê°ì • ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ ê°ì • ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ê°ì • ë¶„ì„ ìˆ˜í–‰
        negative_sentences = []
        for candidate in candidates:
            sentence = candidate['sentence']
            
            if sentiment_model and sentiment_tokenizer:
                # ê°ì • ëª¨ë¸ë¡œ ë¶„ì„
                inputs = sentiment_tokenizer(sentence, return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = sentiment_model(**inputs)
                    probabilities = torch.softmax(outputs.logits, dim=1)
                    sentiment_score = probabilities[0][1].item()  # ë¶€ì • í™•ë¥ 
                
                # ë¶€ì • í™•ë¥ ì´ ë†’ì€ ë¬¸ì¥ ì„ íƒ (ì„ê³„ê°’ ë” ë‚®ì¶¤)
                if sentiment_score > 0.15:  # 15% ì´ìƒ ë¶€ì • (ë” ë‚®ì€ ì„ê³„ê°’)
                    negative_sentences.append({
                        "sentence": sentence,
                        "sentiment_score": sentiment_score
                    })
                    print(f"ğŸŸ  ê°ì • ë¶„ì„ ê²°ê³¼: {sentence[:30]}... (ë¶€ì • í™•ë¥ : {sentiment_score:.3f})")
                else:
                    print(f"ğŸŸ  ê°ì • ë¶„ì„ ì œì™¸: {sentence[:30]}... (ë¶€ì • í™•ë¥ : {sentiment_score:.3f})")
            else:
                # ê°ì • ëª¨ë¸ì´ ì—†ìœ¼ë©´ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë¶„ì„
                # ëª¨ë“  ë¬¸ì¥ì„ í›„ë³´ë¡œ ì¶”ê°€ (LLMì´ íŒë‹¨í•˜ë„ë¡)
                negative_sentences.append({
                    "sentence": sentence,
                    "sentiment_score": 0.3  # ê¸°ë³¸ê°’ (ë” ë‚®ê²Œ ì„¤ì •)
                })
                print(f"ğŸŸ  ê¸°ë³¸ ë¶„ì„: {sentence[:30]}... (ê¸°ë³¸ ì ìˆ˜: 0.3)")
        
        # ë§Œì•½ ê°ì • ë¶„ì„ìœ¼ë¡œ í›„ë³´ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ë¬¸ì¥ì„ í›„ë³´ë¡œ ì¶”ê°€
        if not negative_sentences:
            print("ğŸŸ  ê°ì • ë¶„ì„ í›„ë³´ê°€ ì—†ì–´ì„œ ëª¨ë“  ë¬¸ì¥ì„ í›„ë³´ë¡œ ì¶”ê°€")
            for candidate in candidates:
                negative_sentences.append({
                    "sentence": candidate['sentence'],
                    "sentiment_score": 0.2  # ê¸°ë³¸ê°’
                })
        
        print(f"ğŸŸ  ê°ì • ë¶„ì„ í›„ë³´ ë¬¸ì¥ ìˆ˜: {len(negative_sentences)}")
        
        # ë¶€ì • í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬
        negative_sentences.sort(key=lambda x: x["sentiment_score"], reverse=True)
        
        # ìƒìœ„ 5ê°œ ë¬¸ì¥ë§Œ ì„ íƒ
        top_negative = negative_sentences[:5]
        print(f"ğŸŸ  ìƒìœ„ 5ê°œ í›„ë³´ ë¬¸ì¥ ì„ íƒ: {len(top_negative)}ê°œ")
        
        # í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì„¸ë¶€ ë¶„ì„
        if top_negative:
            sentences_json = json.dumps([c['sentence'] for c in top_negative], ensure_ascii=False, indent=2)
            # ë¶€ì • í‚¤ì›Œë“œ ì¶”ê°€ (ê³¼ì¥ í‘œí˜„ í¬í•¨)
            negative_keywords = [
                # ì¼ë°˜ ë¶€ì • í‚¤ì›Œë“œ
                "ì‹¤íŒ¨", "ì–´ë ¤ì›€", "ë¬¸ì œ", "ì‹¤ìˆ˜", "ë¶€ì¡±", "ë¯¸í¡", "ì‹¤ë§", "ì¢Œì ˆ", "í˜ë“¤ì—ˆë‹¤", 
                "ë§‰ë§‰í–ˆë‹¤", "ë‹¹í™©í–ˆë‹¤", "í˜¼ë€ìŠ¤ëŸ¬ì› ë‹¤", "ë¶ˆì•ˆí–ˆë‹¤", "ê±±ì •í–ˆë‹¤", "ë‚˜ìœ", 
                "ì•ˆì¢‹ì€", "ë¶€ì¡±í•œ", "ë¯¸í¡í•œ", "ì‹¤íŒ¨í•œ", "ì–´ë ¤ì› ë‹¤", "í˜ë“¤ì—ˆë‹¤", "ë§‰ë§‰í–ˆë‹¤", 
                "ë‹¹í™©í–ˆë‹¤", "í˜¼ë€ìŠ¤ëŸ¬ì› ë‹¤", "ë¶ˆì•ˆí–ˆë‹¤", "ê±±ì •í–ˆë‹¤", "ì‹¤ë§í–ˆë‹¤", "ì¢Œì ˆí–ˆë‹¤",
                # ê³¼ì¥ í‘œí˜„ í‚¤ì›Œë“œ
                "ìµœê³ ì˜", "ìµœìƒì˜", "ì™„ë²½í•œ", "ì™„ì „í•œ", "ì™„ì„±ëœ", "íƒì›”í•œ", "ë›°ì–´ë‚œ", "í›Œë¥­í•œ",
                "ë§¤ìš°", "ì •ë§", "ë„ˆë¬´", "ì—„ì²­", "ëŒ€ë‹¨íˆ", "ê·¹ë„ë¡œ", "ê·¹í•œ", "ìµœëŒ€í•œ", "ìµœì„ ì„",
                "ì™„ë²½í•˜ê²Œ", "ì™„ì „íˆ", "ì™„ì„±ë„", "íƒì›”í•˜ê²Œ", "ë›°ì–´ë‚˜ê²Œ", "í›Œë¥­í•˜ê²Œ",
                "100%", "ì™„ë²½", "ì™„ì „", "ìµœê³ ", "ìµœìƒ", "íƒì›”", "ë›°ì–´ë‚¨", "í›Œë¥­í•¨",
                "ì••ë„ì ", "ì••ë„ì ìœ¼ë¡œ", "ì••ë„í•˜ë‹¤", "ì••ë„í–ˆë‹¤", "ì••ë„ì ì¸",
                "ë¬´ì¡°ê±´", "ë°˜ë“œì‹œ", "ì ˆëŒ€", "ì ˆëŒ€ì ìœ¼ë¡œ", "ì ˆëŒ€ì "
            ]
            prompt = get_orange_prompt(negative_keywords, top_negative, full_text)
            print(f"ğŸŸ  í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ, LLM í˜¸ì¶œ ì¤‘...")
            
            # LLM í˜¸ì¶œ
            response = await llm.ainvoke(prompt)
            print(f"ğŸŸ  LLM ì‘ë‹µ ë°›ìŒ: {len(response.content)} ë¬¸ì")
            
            try:
                result = json.loads(response.content)
                highlights = result.get("highlights", [])
                print(f"ğŸŸ  íŒŒì‹±ëœ í•˜ì´ë¼ì´íŠ¸ ìˆ˜: {len(highlights)}")
                
                # ê°ì • ì ìˆ˜ ì¶”ê°€
                for highlight in highlights:
                    for neg_sent in top_negative:
                        if highlight["sentence"] == neg_sent["sentence"]:
                            highlight["sentiment_score"] = neg_sent["sentiment_score"]
                            break
                
                print(f"ğŸŸ  ìµœì¢… ì£¼í™©ìƒ‰ í•˜ì´ë¼ì´íŠ¸ ìˆ˜: {len(highlights)}")
                return highlights
            except json.JSONDecodeError:
                print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {response.content}")
                return []
        
        return []
        
    except Exception as e:
        print(f"ì˜¤ë Œì§€ìƒ‰ ê°ì • ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return []

def perform_advanced_highlighting(state: Dict[str, Any]) -> Dict[str, Any]:
    """ê³ ê¸‰ í•˜ì´ë¼ì´íŒ… ìˆ˜í–‰ ë…¸ë“œ (LLM ê¸°ë°˜)"""
    resume_content = state.get("resume_content", "")
    highlight_criteria = state.get("highlight_criteria", {})
    jobpost_id = state.get("jobpost_id")
    company_id = state.get("company_id")
    
    # ê° ìƒ‰ìƒë³„ë¡œ í•˜ì´ë¼ì´íŒ… ìˆ˜í–‰
    highlights = {
        "yellow": [],
        "red": [],
        "orange": [],
        "purple": [],
        "blue": []
    }
    
    # ë¹„ë™ê¸° ë¶„ì„ì„ ìœ„í•œ ì¤€ë¹„
    import asyncio
    
    async def run_all_analyses():
        tasks = []
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±
        for color, criteria in highlight_criteria.items():
            # í‚¤ì›Œë“œ ëŒ€ì‹  ë¹ˆ ë°°ì—´ ì „ë‹¬ (LLMì´ ë¬¸ë§¥ìœ¼ë¡œ íŒë‹¨)
            task = analyze_category_with_llm(resume_content, color, [])
            tasks.append((color, task))
        
        # ëª¨ë“  ë¶„ì„ ì‹¤í–‰
        results = {}
        for color, task in tasks:
            try:
                result = await task
                results[color] = result
                print(f"âœ… {color} ë¶„ì„ ì™„ë£Œ: {len(result)}ê°œ ê²°ê³¼")
            except Exception as e:
                print(f"âŒ ë¶„ì„ ì˜¤ë¥˜ ({color}): {str(e)}")
                results[color] = []
        
        return results
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, run_all_analyses())
                highlights = future.result()
        else:
            highlights = loop.run_until_complete(run_all_analyses())
    except Exception as e:
        print(f"í•˜ì´ë¼ì´íŒ… ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ fallback
        highlights = perform_basic_highlighting(resume_content, highlight_criteria)
    
    # ì „ì²´ í•˜ì´ë¼ì´íŠ¸ í†µí•© (ìƒ‰ìƒë³„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘)
    all_highlights = []
    color_to_category = {
        "yellow": "value_fit",
        "red": "mismatch", 
        "orange": "negative_tone",
        "purple": "experience",
        "blue": "skill_fit"
    }
    
    for color, color_highlights in highlights.items():
        print(f"ğŸ”„ {color} í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ ì¤‘: {len(color_highlights)}ê°œ")
        for highlight in color_highlights:
            # ìƒ‰ìƒë³„ë¡œ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ëª… ì„¤ì •
            category = color_to_category.get(color, color)
            all_highlights.append({
                **highlight,
                "category": category,  # ì˜ë¯¸ì  ì¹´í…Œê³ ë¦¬ëª…ìœ¼ë¡œ ì„¤ì •
                "color": color  # ìƒ‰ìƒ ì •ë³´ë„ ìœ ì§€
            })
    
    # ğŸ†• ì „í™˜ì–´ë¥¼ ê³ ë ¤í•œ ë¶€ì • í•˜ì´ë¼ì´íŒ… í•„í„°ë§
    filtered_highlights = filter_negative_highlights_with_transitions(all_highlights, resume_content)
    
    # í•„í„°ë§ëœ ê²°ê³¼ë¥¼ ìƒ‰ìƒë³„ë¡œ ë‹¤ì‹œ ë¶„ë¥˜
    filtered_by_color = {
        "yellow": [],
        "red": [],
        "orange": [],
        "purple": [],
        "blue": []
    }
    
    for highlight in filtered_highlights:
        color = highlight.get("color", "")
        if color in filtered_by_color:
            # color í‚¤ ì œê±°í•˜ê³  ì›ë³¸ í˜•íƒœë¡œ ë³µì› (categoryëŠ” ìœ ì§€)
            highlight_copy = {k: v for k, v in highlight.items() if k != "color"}
            filtered_by_color[color].append(highlight_copy)
    
    return {
        **state,
        "highlights": filtered_by_color,
        "all_highlights": filtered_highlights,
        "next": "validate_highlights"
    }

def perform_basic_highlighting(resume_content: str, highlight_criteria: Dict[str, Any]) -> Dict[str, Any]:
    """ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­ (fallbackìš©) - í‚¤ì›Œë“œ ì—†ì´ ë¹ˆ ê²°ê³¼ ë°˜í™˜"""
    highlights = {
        "yellow": [],
        "red": [],
        "orange": [],
        "purple": [],
        "blue": []
    }
    
    # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ê²°ê³¼ ë°˜í™˜
    print("í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„í™œì„±í™”ë¨ - LLM ê¸°ë°˜ ë¶„ì„ë§Œ ì‚¬ìš©")
    
    return highlights

def validate_highlights(state: Dict[str, Any]) -> Dict[str, Any]:
    """í•˜ì´ë¼ì´íŒ… ê²°ê³¼ ê²€ì¦ ë…¸ë“œ"""
    highlights = state.get("highlights", {})
    all_highlights = state.get("all_highlights", [])
    
    # ê²€ì¦ ê²°ê³¼
    validation_result = {
        "total_highlights": len(all_highlights),
        "color_distribution": {},
        "quality_score": 0.0,
        "issues": []
    }
    
    # ìƒ‰ìƒë³„ ë¶„í¬ ê³„ì‚°
    for color, color_highlights in highlights.items():
        validation_result["color_distribution"][color] = len(color_highlights)
    
    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (í•˜ì´ë¼ì´íŠ¸ ê°œìˆ˜ì™€ ë¶„í¬ ê¸°ë°˜)
    total_sentences = len(re.split(r'[.!?]\s+', state.get("resume_content", "")))
    if total_sentences > 0:
        highlight_ratio = len(all_highlights) / total_sentences
        if 0.1 <= highlight_ratio <= 0.4:  # ì ì ˆí•œ ë¹„ìœ¨
            validation_result["quality_score"] = 0.9
        elif 0.05 <= highlight_ratio <= 0.5:  # í—ˆìš© ê°€ëŠ¥í•œ ë¹„ìœ¨
            validation_result["quality_score"] = 0.7
        else:
            validation_result["quality_score"] = 0.5
            validation_result["issues"].append("í•˜ì´ë¼ì´íŠ¸ ë¹„ìœ¨ì´ ì ì ˆí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
    
    # ìƒ‰ìƒë³„ ê· í˜• í™•ì¸
    color_counts = list(validation_result["color_distribution"].values())
    if max(color_counts) > 0:
        balance_ratio = min(color_counts) / max(color_counts)
        if balance_ratio < 0.2:
            validation_result["issues"].append("í•˜ì´ë¼ì´íŠ¸ ìƒ‰ìƒ ë¶„í¬ê°€ ë¶ˆê· í˜•í•©ë‹ˆë‹¤")
    
    return {
        **state,
        "validation_result": validation_result,
        "next": "finalize_results"
    }

def finalize_results(state: Dict[str, Any]) -> Dict[str, Any]:
    """ìµœì¢… ê²°ê³¼ ì •ë¦¬"""
    try:
        highlights = state.get("highlights", {})
        
        # ê²°ê³¼ ì •ë¦¬ ë¡œì§
        result = {
            "yellow": highlights.get("yellow", []),
            "red": highlights.get("red", []),
            "orange": highlights.get("orange", []),
            "purple": highlights.get("purple", []),
            "blue": highlights.get("blue", []),
            "highlights": highlights.get("highlights", []),
            "metadata": state.get("metadata", {})
        }
        
        # ìƒ‰ìƒë³„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        color_mapping = {
            "yellow": "value_fit",
            "red": "risk",
            "orange": "negative_tone",
            "purple": "experience",
            "blue": "skill_fit"
        }
        
        # í†µí•© í•˜ì´ë¼ì´íŠ¸ ë°°ì—´ ìƒì„±
        all_highlights = []
        for color, category in color_mapping.items():
            color_highlights = highlights.get(color, [])
            for highlight in color_highlights:
                all_highlights.append({
                    **highlight,
                    "category": category,
                    "color": color
                })
        
        result["all_highlights"] = all_highlights
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        metadata = state.get("metadata", {})
        metadata.update({
            "total_highlights": len(all_highlights),
            "color_distribution": {
                color: len(highlights.get(color, [])) 
                for color in ["yellow", "red", "orange", "purple", "blue"]
            }
        })
        
        result["metadata"] = metadata
        
        print(f"âœ… ìµœì¢… ê²°ê³¼ ì •ë¦¬ ì™„ë£Œ: ì´ {len(all_highlights)}ê°œ í•˜ì´ë¼ì´íŠ¸")
        return result
        
    except Exception as e:
        print(f"âŒ ìµœì¢… ê²°ê³¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return {
            "yellow": [],
            "red": [],
            "orange": [],
            "purple": [],
            "blue": [],
            "highlights": [],
            "all_highlights": [],
            "metadata": {"error": str(e)}
        }

def build_highlight_workflow() -> StateGraph:
    """í˜•ê´‘íœ í•˜ì´ë¼ì´íŒ… ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±"""
    workflow = StateGraph(Dict[str, Any])
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("analyze_content", analyze_resume_content)
    workflow.add_node("generate_criteria", generate_highlight_criteria)
    workflow.add_node("perform_highlighting", perform_advanced_highlighting)
    workflow.add_node("validate_highlights", validate_highlights)
    workflow.add_node("finalize_results", finalize_results)
    
    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("analyze_content")
    
    # ì—£ì§€ ì—°ê²°
    workflow.add_edge("analyze_content", "generate_criteria")
    workflow.add_edge("generate_criteria", "perform_highlighting")
    workflow.add_edge("perform_highlighting", "validate_highlights")
    workflow.add_edge("validate_highlights", "finalize_results")
    workflow.add_edge("finalize_results", END)
    
    return workflow.compile()

# ì›Œí¬í”Œë¡œìš° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
highlight_workflow = build_highlight_workflow()

def process_highlight_workflow(
    resume_content: str,
    jobpost_id: int = None,
    company_id: int = None
) -> Dict[str, Any]:
    """í˜•ê´‘íœ í•˜ì´ë¼ì´íŒ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        "resume_content": resume_content,
        "jobpost_id": jobpost_id,
        "company_id": company_id
    }
    
    try:
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = highlight_workflow.invoke(initial_state)
        return result.get("final_result", {})
    except Exception as e:
        print(f"í•˜ì´ë¼ì´íŒ… ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {str(e)}")
        return {
            "yellow": [],
            "red": [],
            "orange": [],
            "purple": [],
            "blue": [],
            "highlights": [],
            "metadata": {
                "total_highlights": 0,
                "quality_score": 0.0,
                "color_distribution": {},
                "issues": [f"ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {str(e)}"]
            }
        } 