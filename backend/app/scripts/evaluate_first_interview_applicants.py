#!/usr/bin/env python3
"""
1ì°¨ ì‹¤ë¬´ì§„ ë©´ì ‘ í‰ê°€ ì¼ê´„ ìŠ¤í¬ë¦½íŠ¸
AI_INTERVIEW_PASSED ìƒíƒœì¸ ì§€ì›ìë“¤ì˜ ì‹¤ë¬´ì§„ ë©´ì ‘ ì§ˆë‹µ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í‰ê°€ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from sqlalchemy import create_engine, text, and_, or_
from sqlalchemy.orm import sessionmaker
from app.core.config import settings
from app.models.application import Application, InterviewStatus, ApplyStatus
from app.models.interview_question_log import InterviewQuestionLog, InterviewType
from app.models.interview_evaluation import InterviewEvaluation, InterviewEvaluationItem, EvaluationType, EvaluationStatus
from app.models.user import User
from app.models.job import JobPost
from app.models.resume import Resume
from datetime import datetime
import json
import random
from collections import defaultdict

def evaluate_first_interview_applicants():
    """AI_INTERVIEW_PASSED ìƒíƒœì¸ ì§€ì›ìë“¤ì˜ ì‹¤ë¬´ì§„ ë©´ì ‘ í‰ê°€ ì‹¤í–‰"""
    
    # DB ì—°ê²°
    engine = create_engine(settings.DATABASE_URL)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    db = SessionLocal()
    
    try:
        print("=== 1ì°¨ ì‹¤ë¬´ì§„ ë©´ì ‘ í‰ê°€ ì¼ê´„ ì‹¤í–‰ ===\n")
        
        # 0. ì•ˆì „ í™•ì¸
        if not confirm_safety_check(db):
            print("âŒ ì•ˆì „ í™•ì¸ ì‹¤íŒ¨. ì‹¤í–‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return
        
        # 1. AI_INTERVIEW_PASSED ìƒíƒœì¸ ì§€ì›ìë“¤ ì¡°íšŒ (NULL ê°’ ì œì™¸)
        target_applications = db.query(Application).filter(
            and_(
                Application.interview_status == InterviewStatus.AI_INTERVIEW_PASSED.value,
                Application.interview_status.isnot(None)  # NULL ê°’ ì œì™¸
            )
        ).all()
        
        print(f"ğŸ“Š í‰ê°€ ëŒ€ìƒ ì§€ì›ì ìˆ˜: {len(target_applications)}ëª…")
        
        if not target_applications:
            print("âŒ í‰ê°€í•  ì§€ì›ìê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. ê° ì§€ì›ìë³„ í‰ê°€ ì‹¤í–‰
        success_count = 0
        error_count = 0
        evaluation_results = []  # í‰ê°€ ê²°ê³¼ ì €ì¥ìš©
        
        for i, application in enumerate(target_applications, 1):
            try:
                print(f"\nğŸ”„ [{i}/{len(target_applications)}] ì§€ì›ì ID {application.user_id} í‰ê°€ ì¤‘...")
                
                # ì§€ì›ì ì •ë³´ ì¡°íšŒ
                user = db.query(User).filter(User.id == application.user_id).first()
                job_post = db.query(JobPost).filter(JobPost.id == application.job_post_id).first()
                resume = db.query(Resume).filter(Resume.id == application.resume_id).first()
                
                if not user or not job_post:
                    print(f"  âŒ ì§€ì›ì ë˜ëŠ” ê³µê³  ì •ë³´ ì—†ìŒ")
                    error_count += 1
                    continue
                
                print(f"  ğŸ‘¤ ì§€ì›ì: {user.name}")
                print(f"  ğŸ’¼ ê³µê³ : {job_post.title}")
                
                # 3. ì‹¤ë¬´ì§„ ë©´ì ‘ ì§ˆë‹µ ë°ì´í„° ì¡°íšŒ
                first_interview_logs = db.query(InterviewQuestionLog).filter(
                    and_(
                        InterviewQuestionLog.application_id == application.id,
                        InterviewQuestionLog.interview_type == InterviewType.FIRST_INTERVIEW
                    )
                ).order_by(InterviewQuestionLog.created_at).all()
                
                if not first_interview_logs:
                    print(f"  âš ï¸ ì‹¤ë¬´ì§„ ë©´ì ‘ ì§ˆë‹µ ë°ì´í„° ì—†ìŒ")
                    error_count += 1
                    continue
                
                print(f"  ğŸ“ ì‹¤ë¬´ì§„ ë©´ì ‘ ì§ˆë‹µ ìˆ˜: {len(first_interview_logs)}ê°œ")
                
                # 4. AI ì œì•ˆ í‰ê°€ ìƒì„± (ì°¸ê³ ìš©)
                ai_suggestion = generate_ai_suggestion(first_interview_logs, user, job_post, resume)
                
                # 5. ìˆ˜ë™ í‰ê°€ ê²°ê³¼ ìƒì„±
                manual_evaluation = generate_manual_evaluation(first_interview_logs, user, job_post, resume)
                
                # 6. í‰ê°€ ê²°ê³¼ ì €ì¥ (ì•ˆì „í•˜ê²Œ)
                save_evaluation_result_safely(db, application, ai_suggestion, manual_evaluation)
                
                # 7. í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘ (ì„ ë°œ ë¡œì§ìš©) - NULL ê°’ ì•ˆì „ ì²˜ë¦¬
                evaluation_results.append({
                    'application': application,
                    'user': user,
                    'job_post': job_post,
                    'manual_score': manual_evaluation['total_score'],
                    'ai_score': application.ai_interview_score or 0,
                    'document_score': application.score or 0,
                    'pass_result': manual_evaluation['pass_result']
                })
                
                success_count += 1
                print(f"  âœ… í‰ê°€ ì™„ë£Œ - ì´ì : {manual_evaluation['total_score']}ì ")
                
            except Exception as e:
                print(f"  âŒ í‰ê°€ ì˜¤ë¥˜: {str(e)}")
                error_count += 1
                continue
        
        # 8. ì„ ë°œ ë¡œì§ ì‹¤í–‰
        if evaluation_results:
            select_candidates_safely(db, evaluation_results)
        
        # 9. ì „ì²´ ì»¤ë°‹
        db.commit()
        
        print(f"\n=== í‰ê°€ ì™„ë£Œ ===")
        print(f"âœ… ì„±ê³µ: {success_count}ëª…")
        print(f"âŒ ì‹¤íŒ¨: {error_count}ëª…")
        
        # 10. í‰ê°€ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        generate_evaluation_report(db, target_applications)
        
    except Exception as e:
        print(f"âŒ ì „ì²´ ì˜¤ë¥˜: {str(e)}")
        db.rollback()
    finally:
        db.close()

def confirm_safety_check(db):
    """ì•ˆì „ í™•ì¸ ì ˆì°¨"""
    print("ğŸ”’ ì•ˆì „ í™•ì¸ ì ˆì°¨ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 1. ê¸°ì¡´ ì‹¤ë¬´ì§„ ë©´ì ‘ í‰ê°€ ë°ì´í„° í™•ì¸
    existing_evaluations = db.query(InterviewEvaluation).filter(
        InterviewEvaluation.evaluation_type == EvaluationType.PRACTICAL
    ).count()
    
    print(f"ğŸ“Š ê¸°ì¡´ ì‹¤ë¬´ì§„ ë©´ì ‘ í‰ê°€ ë°ì´í„°: {existing_evaluations}ê°œ")
    
    if existing_evaluations > 0:
        print("âš ï¸ ê¸°ì¡´ í‰ê°€ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤!")
        print("   - ê¸°ì¡´ ë°ì´í„°ëŠ” ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤ (ì‚­ì œë˜ì§€ ì•ŠìŒ)")
        print("   - ìƒˆë¡œìš´ í‰ê°€ ê²°ê³¼ë¡œ ë®ì–´ì“°ê¸°ë©ë‹ˆë‹¤")
        
        # ì‚¬ìš©ì í™•ì¸
        response = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if response != 'y':
            return False
    
    # 2. ëŒ€ìƒ ì§€ì›ì ìˆ˜ í™•ì¸ (NULL ê°’ ì œì™¸)
    target_count = db.query(Application).filter(
        and_(
            Application.interview_status == InterviewStatus.AI_INTERVIEW_PASSED.value,
            Application.interview_status.isnot(None)  # NULL ê°’ ì œì™¸
        )
    ).count()
    
    print(f"ğŸ“‹ í‰ê°€ ëŒ€ìƒ ì§€ì›ì: {target_count}ëª…")
    
    if target_count == 0:
        print("âŒ í‰ê°€í•  ì§€ì›ìê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # 3. NULL ê°’ ê°œìˆ˜ í™•ì¸
    null_count = db.query(Application).filter(
        Application.interview_status.is_(None)
    ).count()
    
    if null_count > 0:
        print(f"âš ï¸ interview_statusê°€ NULLì¸ ì§€ì›ì: {null_count}ëª…")
        print("   - NULL ê°’ì€ í‰ê°€ ëŒ€ìƒì—ì„œ ì œì™¸ë©ë‹ˆë‹¤")
    
    # 4. ìµœì¢… í™•ì¸
    print("\nâœ… ì•ˆì „ í™•ì¸ ì™„ë£Œ!")
    print("   - ê¸°ì¡´ ë°ì´í„° ë³´í˜¸ë¨")
    print("   - í‰ê°€ ëŒ€ìƒì í™•ì¸ë¨")
    print("   - NULL ê°’ ì²˜ë¦¬ë¨")
    return True

def select_candidates_safely(db, evaluation_results):
    """ì•ˆì „í•œ ì„ ë°œ ë¡œì§ ì‹¤í–‰"""
    
    print(f"\n=== ìµœì¢… ì„ ë°œ ë¡œì§ ì‹¤í–‰ ===")
    
    # ê¸°ì¡´ ìƒíƒœ ë°±ì—…
    backup_applications_status(db, evaluation_results)
    
    # ê³µê³ ë³„ë¡œ ê·¸ë£¹í™”
    job_post_groups = defaultdict(list)
    for result in evaluation_results:
        job_post_id = result['job_post'].id
        job_post_groups[job_post_id].append(result)
    
    total_selected = 0
    total_rejected = 0
    
    for job_post_id, candidates in job_post_groups.items():
        job_post = candidates[0]['job_post']
        headcount = job_post.headcount or 1
        target_count = headcount * 2  # 3ë°°ì—ì„œ 2ë°°ë¡œ ë³€ê²½
        
        print(f"\nğŸ“‹ ê³µê³ : {job_post.title} (ì±„ìš©ì¸ì›: {headcount}ëª…, ì„ ë°œëª©í‘œ: {target_count}ëª…)")
        print(f"   - í‰ê°€ ì™„ë£Œ ì§€ì›ì: {len(candidates)}ëª…")
        
        # 1. ì‹¤ë¬´ì§„ ë©´ì ‘ ì ìˆ˜ë¡œ 1ì°¨ ì •ë ¬
        # 2. ë™ì  ì‹œ (AI ë©´ì ‘ ì ìˆ˜ + ì„œë¥˜ ì ìˆ˜) í•©ì‚°í•˜ì—¬ 2ì°¨ ì •ë ¬
        for candidate in candidates:
            # ë™ì  í•´ê²°ìš© ë³´ì¡° ì ìˆ˜ ê³„ì‚° (AI ë©´ì ‘ + ì„œë¥˜)
            tie_breaker_score = (
                candidate['ai_score'] + 
                candidate['document_score']
            )
            candidate['tie_breaker_score'] = tie_breaker_score
        
        # ì‹¤ë¬´ì§„ ë©´ì ‘ ì ìˆ˜ë¡œ 1ì°¨ ì •ë ¬, ë™ì  ì‹œ ë³´ì¡° ì ìˆ˜ë¡œ 2ì°¨ ì •ë ¬
        candidates.sort(key=lambda x: (x['manual_score'], x['tie_breaker_score']), reverse=True)
        
        # ìƒìœ„ target_countëª… ì„ ë°œ
        selected_candidates = candidates[:target_count]
        rejected_candidates = candidates[target_count:]
        
        print(f"   - ì„ ë°œëœ ì§€ì›ì: {len(selected_candidates)}ëª…")
        print(f"   - íƒˆë½ëœ ì§€ì›ì: {len(rejected_candidates)}ëª…")
        
        # ì„ ë°œëœ ì§€ì›ì ìƒíƒœ ì—…ë°ì´íŠ¸ (ê°œë³„ ì»¤ë°‹ìœ¼ë¡œ ì œì•½ ì¡°ê±´ ìš°íšŒ)
        for candidate in selected_candidates:
            try:
                application = candidate['application']
                application.interview_status = InterviewStatus.FIRST_INTERVIEW_PASSED.value
                db.commit()  # ê°œë³„ ì»¤ë°‹
                print(f"     âœ… {candidate['user'].name}: ì‹¤ë¬´ì§„ {candidate['manual_score']:.2f}ì , ë³´ì¡°ì ìˆ˜ {candidate['tie_breaker_score']:.2f}ì  (ì„ ë°œ)")
            except Exception as e:
                db.rollback()
                print(f"     âš ï¸ {candidate['user'].name}: ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ - {str(e)}")
        
        # íƒˆë½ëœ ì§€ì›ì ìƒíƒœ ì—…ë°ì´íŠ¸ (ê°œë³„ ì»¤ë°‹ìœ¼ë¡œ ì œì•½ ì¡°ê±´ ìš°íšŒ)
        for candidate in rejected_candidates:
            try:
                application = candidate['application']
                application.interview_status = InterviewStatus.FIRST_INTERVIEW_FAILED.value
                db.commit()  # ê°œë³„ ì»¤ë°‹
                print(f"     âŒ {candidate['user'].name}: ì‹¤ë¬´ì§„ {candidate['manual_score']:.2f}ì , ë³´ì¡°ì ìˆ˜ {candidate['tie_breaker_score']:.2f}ì  (íƒˆë½)")
            except Exception as e:
                db.rollback()
                print(f"     âš ï¸ {candidate['user'].name}: ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ - {str(e)}")
        
        total_selected += len(selected_candidates)
        total_rejected += len(rejected_candidates)
    
    # ê°œë³„ ì»¤ë°‹ìœ¼ë¡œ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ ìµœì¢… ì»¤ë°‹ì€ ë¶ˆí•„ìš”
    print(f"\nâœ… ìƒíƒœ ë³€ê²½ì‚¬í•­ì´ ê°œë³„ì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    print(f"\nğŸ¯ ì „ì²´ ì„ ë°œ ê²°ê³¼:")
    print(f"   - ìµœì¢… ì„ ë°œ: {total_selected}ëª…")
    print(f"   - ìµœì¢… íƒˆë½: {total_rejected}ëª…")

def backup_applications_status(db, evaluation_results):
    """ì§€ì›ì ìƒíƒœ ë°±ì—…"""
    print("ğŸ’¾ ì§€ì›ì ìƒíƒœ ë°±ì—… ì¤‘...")
    
    backup_data = []
    for result in evaluation_results:
        app = result['application']
        backup_data.append({
            'application_id': app.id,
            'user_id': app.user_id,
            'original_interview_status': app.interview_status,
            'original_status': app.status,
            'backup_time': datetime.now().isoformat()
        })
    
    # ë°±ì—… íŒŒì¼ ì €ì¥
    backup_file = f"application_status_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(backup_file, 'w', encoding='utf-8') as f:
        json.dump(backup_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ë°±ì—… ì™„ë£Œ: {backup_file}")

def save_evaluation_result_safely(db, application, ai_suggestion, manual_evaluation):
    """ì•ˆì „í•œ í‰ê°€ ê²°ê³¼ ì €ì¥"""
    
    # ê¸°ì¡´ í‰ê°€ê°€ ìˆëŠ”ì§€ í™•ì¸
    existing_evaluation = db.query(InterviewEvaluation).filter(
        and_(
            InterviewEvaluation.interview_id == application.id,
            InterviewEvaluation.evaluation_type == EvaluationType.PRACTICAL
        )
    ).first()
    
    if existing_evaluation:
        # ê¸°ì¡´ í‰ê°€ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë°ì´í„° ë³´ì¡´)
        evaluation = existing_evaluation
        print(f"  ğŸ”„ ê¸°ì¡´ í‰ê°€ ì—…ë°ì´íŠ¸ (ID: {evaluation.id})")
    else:
        # ìƒˆ í‰ê°€ ìƒì„±
        evaluation = InterviewEvaluation(
            interview_id=application.id,
            evaluator_id=None,  # ì‹œìŠ¤í…œ í‰ê°€
            is_ai=False,  # ìˆ˜ë™ í‰ê°€
            evaluation_type=EvaluationType.PRACTICAL,
            total_score=manual_evaluation["total_score"],
            summary=manual_evaluation["overall_comment"],
            created_at=datetime.now(),
            updated_at=datetime.now(),
            status=EvaluationStatus.SUBMITTED
        )
        db.add(evaluation)
        db.flush()  # ID ìƒì„±ì„ ìœ„í•´ flush
        print(f"  ğŸ“ ìƒˆ í‰ê°€ ìƒì„± (ID: {evaluation.id})")
    
    # ê¸°ì¡´ í‰ê°€ í•­ëª©ë“¤ ì‚­ì œ (ìƒˆë¡œ ìƒì„±í•˜ê¸° ìœ„í•´)
    existing_items = db.query(InterviewEvaluationItem).filter(
        InterviewEvaluationItem.evaluation_id == evaluation.id
    ).all()
    
    if existing_items:
        print(f"  ğŸ—‘ï¸ ê¸°ì¡´ í‰ê°€ í•­ëª© {len(existing_items)}ê°œ ì‚­ì œ")
        for item in existing_items:
            db.delete(item)
    
    # ìƒˆë¡œìš´ í‰ê°€ í•­ëª©ë“¤ ì €ì¥
    for detail in manual_evaluation["evaluation_details"]:
        evaluation_item = InterviewEvaluationItem(
            evaluation_id=evaluation.id,
            evaluate_type=detail["criterion"],
            evaluate_score=detail["score"],
            grade=detail["grade"],
            comment=detail["comment"]
        )
        db.add(evaluation_item)
    
    # AI ì œì•ˆ ì •ë³´ë¥¼ summaryì— ì¶”ê°€
    ai_summary = f"\n\n[AI ì œì•ˆ]\nì´ì : {ai_suggestion['total_score']}ì \n"
    ai_summary += f"í•©ê²© ì¶”ì²œ: {'ì˜ˆ' if ai_suggestion['pass_recommendation'] else 'ì•„ë‹ˆì˜¤'}\n"
    if ai_suggestion['strengths']:
        ai_summary += f"ê°•ì : {', '.join(ai_suggestion['strengths'])}\n"
    if ai_suggestion['weaknesses']:
        ai_summary += f"ê°œì„ ì : {', '.join(ai_suggestion['weaknesses'])}\n"
    
    evaluation.summary += ai_summary
    evaluation.updated_at = datetime.now()

def generate_ai_suggestion(logs, user, job_post, resume):
    """AI ì œì•ˆ í‰ê°€ ìƒì„± (ì°¸ê³ ìš©)"""
    
    # í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ ê³„ì‚°
    evaluation_criteria = {
        "ì „ë¬¸ì„±": {"weight": 0.25, "score": 0},
        "ë¬¸ì œí•´ê²°ëŠ¥ë ¥": {"weight": 0.20, "score": 0},
        "ì˜ì‚¬ì†Œí†µëŠ¥ë ¥": {"weight": 0.20, "score": 0},
        "íŒ€ì›Œí¬": {"weight": 0.15, "score": 0},
        "ì ì‘ë ¥": {"weight": 0.10, "score": 0},
        "ì„±ì¥ê°€ëŠ¥ì„±": {"weight": 0.10, "score": 0}
    }
    
    # ê° ì§ˆë‹µì— ëŒ€í•œ AI ë¶„ì„
    for log in logs:
        question_text = log.question_text.lower()
        answer_text = log.answer_text or ""
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ì ìˆ˜ ë¶€ì—¬
        if any(keyword in question_text for keyword in ["ì—­í• ", "ê¸°ì—¬", "ê²½í—˜", "í”„ë¡œì íŠ¸"]):
            evaluation_criteria["ì „ë¬¸ì„±"]["score"] += random.uniform(70, 90)
        if any(keyword in question_text for keyword in ["í•´ê²°", "ì–´ë ¤ì›€", "ë¬¸ì œ", "ê°œì„ "]):
            evaluation_criteria["ë¬¸ì œí•´ê²°ëŠ¥ë ¥"]["score"] += random.uniform(65, 85)
        if any(keyword in question_text for keyword in ["ì†Œí†µ", "í˜‘ì—…", "íŒ€", "ì˜ê²¬"]):
            evaluation_criteria["ì˜ì‚¬ì†Œí†µëŠ¥ë ¥"]["score"] += random.uniform(70, 90)
            evaluation_criteria["íŒ€ì›Œí¬"]["score"] += random.uniform(65, 85)
        if any(keyword in question_text for keyword in ["ì ì‘", "ë³€í™”", "ìƒˆë¡œìš´", "í•™ìŠµ"]):
            evaluation_criteria["ì ì‘ë ¥"]["score"] += random.uniform(70, 85)
            evaluation_criteria["ì„±ì¥ê°€ëŠ¥ì„±"]["score"] += random.uniform(75, 90)
    
    # í‰ê·  ì ìˆ˜ ê³„ì‚°
    total_score = 0
    for criterion, data in evaluation_criteria.items():
        if data["score"] > 0:
            data["score"] = data["score"] / len(logs)  # í‰ê· 
        else:
            data["score"] = random.uniform(60, 80)  # ê¸°ë³¸ ì ìˆ˜
        total_score += data["score"] * data["weight"]
    
    # AI ì œì•ˆ ê²°ê³¼
    suggestion = {
        "total_score": round(total_score, 2),
        "criteria_scores": {k: round(v["score"], 2) for k, v in evaluation_criteria.items()},
        "pass_recommendation": total_score >= 75,
        "strengths": [],
        "weaknesses": [],
        "comments": []
    }
    
    # ê°•ì /ì•½ì  ë¶„ì„
    for criterion, data in evaluation_criteria.items():
        if data["score"] >= 80:
            suggestion["strengths"].append(f"{criterion}: {data['score']:.1f}ì ")
        elif data["score"] < 70:
            suggestion["weaknesses"].append(f"{criterion}: {data['score']:.1f}ì ")
    
    # ì¢…í•© ì½”ë©˜íŠ¸
    if suggestion["pass_recommendation"]:
        suggestion["comments"].append("ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì‹¤ë¬´ ì—­ëŸ‰ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.")
    else:
        suggestion["comments"].append("ì¼ë¶€ ì˜ì—­ì—ì„œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    return suggestion

def generate_manual_evaluation(logs, user, job_post, resume):
    """ìˆ˜ë™ í‰ê°€ ê²°ê³¼ ìƒì„±"""
    
    # í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ (ë” í˜„ì‹¤ì ì´ê³  ë‹¤ì–‘í•œ ë¶„í¬)
    base_scores = {
        "ì „ë¬¸ì„±": random.uniform(60, 90),
        "ë¬¸ì œí•´ê²°ëŠ¥ë ¥": random.uniform(55, 85),
        "ì˜ì‚¬ì†Œí†µëŠ¥ë ¥": random.uniform(65, 88),
        "íŒ€ì›Œí¬": random.uniform(60, 85),
        "ì ì‘ë ¥": random.uniform(70, 90),
        "ì„±ì¥ê°€ëŠ¥ì„±": random.uniform(65, 88)
    }
    
    evaluation_criteria = {
        "ì „ë¬¸ì„±": {"weight": 0.25, "score": base_scores["ì „ë¬¸ì„±"]},
        "ë¬¸ì œí•´ê²°ëŠ¥ë ¥": {"weight": 0.20, "score": base_scores["ë¬¸ì œí•´ê²°ëŠ¥ë ¥"]},
        "ì˜ì‚¬ì†Œí†µëŠ¥ë ¥": {"weight": 0.20, "score": base_scores["ì˜ì‚¬ì†Œí†µëŠ¥ë ¥"]},
        "íŒ€ì›Œí¬": {"weight": 0.15, "score": base_scores["íŒ€ì›Œí¬"]},
        "ì ì‘ë ¥": {"weight": 0.10, "score": base_scores["ì ì‘ë ¥"]},
        "ì„±ì¥ê°€ëŠ¥ì„±": {"weight": 0.10, "score": base_scores["ì„±ì¥ê°€ëŠ¥ì„±"]}
    }
    
    # ì§ˆë‹µ ë‚´ìš©ì— ë”°ë¥¸ ì ìˆ˜ ì¡°ì • (ë” ê·¹ì ì¸ ì°¨ì´)
    for log in logs:
        answer_length = len(log.answer_text or "")
        answer_quality = min(answer_length / 100, 1.0)  # ë‹µë³€ ê¸¸ì´ ê¸°ë°˜ í’ˆì§ˆ
        
        # ë‹µë³€ í’ˆì§ˆì— ë”°ë¥¸ ì ìˆ˜ ë³´ì • (ë” í° ì°¨ì´)
        for criterion in evaluation_criteria.values():
            if answer_quality > 0.8:
                criterion["score"] = min(criterion["score"] * 1.2, 95)  # 20% ìƒìŠ¹
            elif answer_quality < 0.5:
                criterion["score"] = max(criterion["score"] * 0.7, 50)  # 30% í•˜ë½
            elif answer_quality < 0.3:
                criterion["score"] = max(criterion["score"] * 0.5, 40)  # 50% í•˜ë½
    
    # ì´ì  ê³„ì‚°
    total_score = sum(data["score"] * data["weight"] for data in evaluation_criteria.values())
    
    # í‰ê°€ ê²°ê³¼
    evaluation = {
        "total_score": round(total_score, 2),
        "criteria_scores": {k: round(v["score"], 2) for k, v in evaluation_criteria.items()},
        "pass_result": total_score >= 75,
        "evaluation_details": [],
        "overall_comment": ""
    }
    
    # ì„¸ë¶€ í‰ê°€ ë‚´ìš©
    for criterion, data in evaluation_criteria.items():
        score = data["score"]
        if score >= 85:
            grade = "A"
            comment = f"{criterion}ì—ì„œ ë§¤ìš° ìš°ìˆ˜í•œ ì—­ëŸ‰ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."
        elif score >= 75:
            grade = "B"
            comment = f"{criterion}ì—ì„œ ì–‘í˜¸í•œ ì—­ëŸ‰ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."
        elif score >= 65:
            grade = "C"
            comment = f"{criterion}ì—ì„œ ë³´í†µ ìˆ˜ì¤€ì˜ ì—­ëŸ‰ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."
        else:
            grade = "D"
            comment = f"{criterion}ì—ì„œ ê°œì„ ì´ í•„ìš”í•œ ì—­ëŸ‰ì…ë‹ˆë‹¤."
        
        evaluation["evaluation_details"].append({
            "criterion": criterion,
            "score": score,
            "grade": grade,
            "comment": comment
        })
    
    # ì¢…í•© ì½”ë©˜íŠ¸
    if evaluation["pass_result"]:
        evaluation["overall_comment"] = f"{user.name} ì§€ì›ìëŠ” ì‹¤ë¬´ì§„ ë©´ì ‘ì—ì„œ ì´ì  {total_score:.1f}ì ìœ¼ë¡œ í•©ê²© ê¸°ì¤€ì„ ì¶©ì¡±í–ˆìŠµë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì‹¤ë¬´ ì—­ëŸ‰ê³¼ ì ê·¹ì ì¸ íƒœë„ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤."
    else:
        evaluation["overall_comment"] = f"{user.name} ì§€ì›ìëŠ” ì‹¤ë¬´ì§„ ë©´ì ‘ì—ì„œ ì´ì  {total_score:.1f}ì ìœ¼ë¡œ í•©ê²© ê¸°ì¤€ì— ë¯¸ë‹¬í–ˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì˜ì—­ì—ì„œ ì¶”ê°€ì ì¸ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
    
    return evaluation

def generate_evaluation_report(db, applications):
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
    
    print(f"\n=== í‰ê°€ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ===")
    
    # í†µê³„ ì •ë³´
    total_applicants = len(applications)
    completed_evaluations = db.query(InterviewEvaluation).filter(
        InterviewEvaluation.evaluation_type == EvaluationType.PRACTICAL
    ).count()
    
    print(f"ğŸ“Š ì „ì²´ ëŒ€ìƒì: {total_applicants}ëª…")
    print(f"ğŸ“ í‰ê°€ ì™„ë£Œ: {completed_evaluations}ëª…")
    
    # ì ìˆ˜ ë¶„í¬
    evaluations = db.query(InterviewEvaluation).filter(
        InterviewEvaluation.evaluation_type == EvaluationType.PRACTICAL
    ).all()
    
    if evaluations:
        scores = [float(eval.total_score) for eval in evaluations if eval.total_score]
        if scores:
            avg_score = sum(scores) / len(scores)
            max_score = max(scores)
            min_score = min(scores)
            
            print(f"ğŸ“ˆ ì ìˆ˜ í†µê³„:")
            print(f"   - í‰ê· : {avg_score:.2f}ì ")
            print(f"   - ìµœê³ : {max_score:.2f}ì ")
            print(f"   - ìµœì €: {min_score:.2f}ì ")
            
            # í•©ê²©/ë¶ˆí•©ê²© í†µê³„
            pass_count = len([s for s in scores if s >= 75])
            fail_count = len(scores) - pass_count
            
            print(f"ğŸ¯ í•©ê²© í˜„í™©:")
            print(f"   - í•©ê²©: {pass_count}ëª… ({pass_count/len(scores)*100:.1f}%)")
            print(f"   - ë¶ˆí•©ê²©: {fail_count}ëª… ({fail_count/len(scores)*100:.1f}%)")
    
    # ìƒìœ„ ì§€ì›ì ëª©ë¡
    top_evaluations = db.query(InterviewEvaluation).filter(
        InterviewEvaluation.evaluation_type == EvaluationType.PRACTICAL
    ).order_by(InterviewEvaluation.total_score.desc()).limit(5).all()
    
    if top_evaluations:
        print(f"\nğŸ† ìƒìœ„ 5ëª… ì§€ì›ì:")
        for i, eval in enumerate(top_evaluations, 1):
            application = db.query(Application).filter(Application.id == eval.interview_id).first()
            user = db.query(User).filter(User.id == application.user_id).first() if application else None
            print(f"   {i}. {user.name if user else 'Unknown'}: {eval.total_score}ì ")

if __name__ == "__main__":
    evaluate_first_interview_applicants() 