#!/usr/bin/env python3
"""
AI ë©´ì ‘ ì˜ìƒ ë¶„ì„ íŒŒì´í”„ë¼ì¸
mp4 ì˜ìƒì—ì„œ ë‹¤ì–‘í•œ í‰ê°€ ë°ì´í„° ì¶”ì¶œ
"""

import cv2
import numpy as np
import librosa
import speech_recognition as sr
from moviepy.editor import VideoFileClip
import mediapipe as mp
from deepface import DeepFace
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime

# MediaPipe ì„¤ì •
mp_face_mesh = mp.solutions.face_mesh
mp_hands = mp.solutions.hands
mp_pose = mp.solutions.pose

class VideoAnalysisPipeline:
    def __init__(self):
        self.face_mesh = mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.hands = mp_hands.Hands(
            max_num_hands=2,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        self.pose = mp_pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.recognizer = sr.Recognizer()
        
    def analyze_video(self, video_path: str) -> Dict[str, Any]:
        """ì˜ìƒ ì „ì²´ ë¶„ì„"""
        try:
            print(f"ğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘: {video_path}")
            
            # 1. ì˜¤ë””ì˜¤ ì¶”ì¶œ ë° ë¶„ì„
            audio_analysis = self._analyze_audio(video_path)
            print("âœ… ì˜¤ë””ì˜¤ ë¶„ì„ ì™„ë£Œ")
            
            # 2. ë¹„ë””ì˜¤ í”„ë ˆì„ ë¶„ì„
            video_analysis = self._analyze_video_frames(video_path)
            print("âœ… ë¹„ë””ì˜¤ í”„ë ˆì„ ë¶„ì„ ì™„ë£Œ")
            
            # 3. ìŒì„± ì¸ì‹ ë° í…ìŠ¤íŠ¸ ë¶„ì„
            text_analysis = self._analyze_speech_text(video_path)
            print("âœ… ìŒì„± ì¸ì‹ ë° í…ìŠ¤íŠ¸ ë¶„ì„ ì™„ë£Œ")
            
            # 4. ê²°ê³¼ í†µí•©
            combined_analysis = {
                **audio_analysis,
                **video_analysis,
                **text_analysis,
                "analysis_timestamp": datetime.now().isoformat(),
                "video_path": video_path
            }
            
            print(f"ğŸ‰ ì˜ìƒ ë¶„ì„ ì™„ë£Œ: {len(combined_analysis)}ê°œ í•­ëª©")
            return combined_analysis
            
        except Exception as e:
            logging.error(f"ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    def _analyze_audio(self, video_path: str) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ ë¶„ì„"""
        try:
            # ì˜¤ë””ì˜¤ ì¶”ì¶œ
            video = VideoFileClip(video_path)
            audio = video.audio
            
            # ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ
            y, sr = librosa.load(audio.filename, sr=None)
            
            # 1. ë§ ì†ë„ ë¶„ì„
            speech_rate = self._calculate_speech_rate(y, sr)
            
            # 2. ìŒì„± ë³¼ë¥¨ ë¶„ì„
            volume_level = self._calculate_volume_level(y)
            
            # 3. ë°œìŒ ì •í™•ë„ (ê¸°ë³¸ê°’)
            pronunciation_score = 0.85
            
            # 4. ì–µì–‘/ê°•ì„¸ ë¶„ì„
            intonation_score = self._calculate_intonation(y, sr)
            
            # 5. ê°ì • ë³€í™” ë¶„ì„
            emotion_variation = self._calculate_emotion_variation(y, sr)
            
            # 6. ë°°ê²½ ì†ŒìŒ ë¶„ì„
            background_noise_level = self._calculate_background_noise(y, sr)
            
            return {
                "speech_rate": speech_rate,
                "volume_level": volume_level,
                "pronunciation_score": pronunciation_score,
                "intonation_score": intonation_score,
                "emotion_variation": emotion_variation,
                "background_noise_level": background_noise_level
            }
            
        except Exception as e:
            logging.error(f"ì˜¤ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_default_audio_analysis()
    
    def _analyze_video_frames(self, video_path: str) -> Dict[str, Any]:
        """ë¹„ë””ì˜¤ í”„ë ˆì„ ë¶„ì„"""
        try:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            # ë¶„ì„í•  í”„ë ˆì„ ìˆ˜ (ì„±ëŠ¥ ê³ ë ¤)
            sample_frames = min(300, total_frames)  # ìµœëŒ€ 300í”„ë ˆì„
            frame_interval = max(1, total_frames // sample_frames)
            
            smile_frequencies = []
            eye_contact_ratios = []
            hand_gestures = []
            nod_counts = []
            posture_changes = []
            eye_aversion_counts = []
            facial_expression_variations = []
            
            frame_count = 0
            processed_frames = 0
            
            while cap.isOpened() and processed_frames < sample_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if frame_count % frame_interval == 0:
                    # RGB ë³€í™˜
                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # 1. ì–¼êµ´ ë©”ì‹œ ë¶„ì„
                    face_results = self.face_mesh.process(rgb_frame)
                    if face_results.multi_face_landmarks:
                        smile_freq, eye_contact, eye_aversion = self._analyze_face_landmarks(
                            face_results.multi_face_landmarks[0], frame
                        )
                        smile_frequencies.append(smile_freq)
                        eye_contact_ratios.append(eye_contact)
                        eye_aversion_counts.append(eye_aversion)
                    
                    # 2. ì†ë™ì‘ ë¶„ì„
                    hand_results = self.hands.process(rgb_frame)
                    if hand_results.multi_hand_landmarks:
                        hand_gesture = self._analyze_hand_gestures(hand_results.multi_hand_landmarks)
                        hand_gestures.append(hand_gesture)
                    
                    # 3. ìì„¸ ë¶„ì„
                    pose_results = self.pose.process(rgb_frame)
                    if pose_results.pose_landmarks:
                        posture_change, nod_count = self._analyze_pose(pose_results.pose_landmarks)
                        posture_changes.append(posture_change)
                        nod_counts.append(nod_count)
                    
                    # 4. í‘œì • ë³€í™” ë¶„ì„
                    facial_expr = self._analyze_facial_expression(frame)
                    facial_expression_variations.append(facial_expr)
                    
                    processed_frames += 1
                
                frame_count += 1
            
            cap.release()
            
            # í‰ê· ê°’ ê³„ì‚°
            return {
                "smile_frequency": np.mean(smile_frequencies) if smile_frequencies else 1.0,
                "eye_contact_ratio": np.mean(eye_contact_ratios) if eye_contact_ratios else 0.8,
                "hand_gesture": np.mean(hand_gestures) if hand_gestures else 0.5,
                "nod_count": int(np.mean(nod_counts)) if nod_counts else 2,
                "posture_changes": int(np.mean(posture_changes)) if posture_changes else 2,
                "eye_aversion_count": int(np.mean(eye_aversion_counts)) if eye_aversion_counts else 1,
                "facial_expression_variation": np.mean(facial_expression_variations) if facial_expression_variations else 0.6
            }
            
        except Exception as e:
            logging.error(f"ë¹„ë””ì˜¤ í”„ë ˆì„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_default_video_analysis()
    
    def _analyze_speech_text(self, video_path: str) -> Dict[str, Any]:
        """ìŒì„± ì¸ì‹ ë° í…ìŠ¤íŠ¸ ë¶„ì„"""
        try:
            # ì˜¤ë””ì˜¤ ì¶”ì¶œ
            video = VideoFileClip(video_path)
            audio = video.audio
            
            # ìŒì„± ì¸ì‹
            with sr.AudioFile(audio.filename) as source:
                audio_data = self.recognizer.record(source)
                text = self.recognizer.recognize_google(audio_data, language='ko-KR')
            
            # í…ìŠ¤íŠ¸ ë¶„ì„
            return self._analyze_text_content(text)
            
        except Exception as e:
            logging.error(f"ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}")
            return self._get_default_text_analysis()
    
    def _calculate_speech_rate(self, y: np.ndarray, sr: int) -> float:
        """ë§ ì†ë„ ê³„ì‚° (ë‹¨ì–´/ë¶„)"""
        try:
            # ìŒì„± êµ¬ê°„ ê²€ì¶œ
            intervals = librosa.effects.split(y, top_db=20)
            
            # ìŒì„± êµ¬ê°„ ê¸¸ì´ ê³„ì‚°
            speech_duration = sum([(end - start) / sr for start, end in intervals])
            
            # ë‹¨ì–´ ìˆ˜ ì¶”ì • (í•œêµ­ì–´ ê¸°ì¤€)
            word_count = len(y) / (sr * 0.3)  # ëŒ€ëµì ì¸ ì¶”ì •
            
            # ë§ ì†ë„ ê³„ì‚°
            speech_rate = (word_count / speech_duration) * 60
            
            return min(max(speech_rate, 80), 200)  # 80-200 ë²”ìœ„ë¡œ ì œí•œ
            
        except:
            return 150.0  # ê¸°ë³¸ê°’
    
    def _calculate_volume_level(self, y: np.ndarray) -> float:
        """ìŒì„± ë³¼ë¥¨ ë ˆë²¨ ê³„ì‚°"""
        try:
            rms = np.sqrt(np.mean(y**2))
            return min(max(rms, 0.1), 1.0)
        except:
            return 0.75
    
    def _calculate_intonation(self, y: np.ndarray, sr: int) -> float:
        """ì–µì–‘/ê°•ì„¸ ë¶„ì„"""
        try:
            # í”¼ì¹˜ ì¶”ì¶œ
            pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
            
            # í”¼ì¹˜ ë³€í™”ëŸ‰ ê³„ì‚°
            pitch_changes = np.diff(pitches, axis=1)
            intonation_score = np.mean(np.abs(pitch_changes))
            
            return min(max(intonation_score, 0.1), 1.0)
        except:
            return 0.6
    
    def _calculate_emotion_variation(self, y: np.ndarray, sr: int) -> float:
        """ê°ì • ë³€í™” ë¶„ì„"""
        try:
            # MFCC íŠ¹ì§• ì¶”ì¶œ
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            
            # MFCC ë³€í™”ëŸ‰ ê³„ì‚°
            mfcc_changes = np.diff(mfccs, axis=1)
            emotion_variation = np.mean(np.abs(mfcc_changes))
            
            return min(max(emotion_variation, 0.1), 1.0)
        except:
            return 0.6
    
    def _calculate_background_noise(self, y: np.ndarray, sr: int) -> float:
        """ë°°ê²½ ì†ŒìŒ ë ˆë²¨ ê³„ì‚°"""
        try:
            # ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„
            spec = np.abs(librosa.stft(y))
            
            # ë°°ê²½ ì†ŒìŒ ì¶”ì •
            noise_level = np.mean(spec[:, :10])  # ì²˜ìŒ 10í”„ë ˆì„ í‰ê· 
            
            return min(max(noise_level, 0.0), 1.0)
        except:
            return 0.1
    
    def _analyze_face_landmarks(self, landmarks, frame) -> tuple:
        """ì–¼êµ´ ëœë“œë§ˆí¬ ë¶„ì„"""
        try:
            # ë¯¸ì†Œ ë¶„ì„ (ì…ê¼¬ë¦¬ ìœ„ì¹˜)
            left_corner = landmarks.landmark[61]
            right_corner = landmarks.landmark[291]
            
            # ì‹œì„  ë¶„ì„ (ëˆˆ ìœ„ì¹˜)
            left_eye = landmarks.landmark[33]
            right_eye = landmarks.landmark[263]
            
            # ê°„ë‹¨í•œ ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ í•„ìš”)
            smile_freq = 1.0 if abs(left_corner.y - right_corner.y) > 0.02 else 0.0
            eye_contact = 0.9 if abs(left_eye.y - right_eye.y) < 0.01 else 0.7
            eye_aversion = 0 if abs(left_eye.y - right_eye.y) < 0.01 else 1
            
            return smile_freq, eye_contact, eye_aversion
            
        except:
            return 1.0, 0.8, 1
    
    def _analyze_hand_gestures(self, hand_landmarks) -> float:
        """ì†ë™ì‘ ë¶„ì„"""
        try:
            # ì†ê°€ë½ ëì ë“¤ì˜ ì›€ì§ì„ ë¶„ì„
            finger_tips = [8, 12, 16, 20]  # ê²€ì§€, ì¤‘ì§€, ì•½ì§€, ìƒˆë¼ ì†ê°€ë½ ë
            
            movements = []
            for tip in finger_tips:
                if tip < len(hand_landmarks.landmark):
                    tip_pos = hand_landmarks.landmark[tip]
                    movements.append(tip_pos.y)
            
            # ì›€ì§ì„ ì •ë„ ê³„ì‚°
            gesture_level = np.std(movements) if movements else 0.5
            
            return min(max(gesture_level, 0.1), 1.0)
            
        except:
            return 0.5
    
    def _analyze_pose(self, pose_landmarks) -> tuple:
        """ìì„¸ ë¶„ì„"""
        try:
            # ê³ ê°œ ìœ„ì¹˜ ë³€í™”
            nose = pose_landmarks.landmark[0]
            left_ear = pose_landmarks.landmark[2]
            right_ear = pose_landmarks.landmark[5]
            
            # ê³ ê°œ ê¸°ìš¸ê¸°
            head_tilt = abs(left_ear.y - right_ear.y)
            
            # ê³ ê°œ ë„ë•ì„ (ê°„ë‹¨í•œ ì¶”ì •)
            nod_count = 1 if head_tilt > 0.05 else 0
            
            # ìì„¸ ë³€í™”
            posture_change = 1 if head_tilt > 0.03 else 0
            
            return posture_change, nod_count
            
        except:
            return 1, 1
    
    def _analyze_facial_expression(self, frame) -> float:
        """í‘œì • ë³€í™” ë¶„ì„"""
        try:
            # DeepFaceë¥¼ ì‚¬ìš©í•œ ê°ì • ë¶„ì„
            result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)
            
            if isinstance(result, list):
                emotions = result[0]['emotion']
            else:
                emotions = result['emotion']
            
            # ê°ì • ë³€í™” ì •ë„ ê³„ì‚°
            emotion_values = list(emotions.values())
            emotion_variation = np.std(emotion_values)
            
            return min(max(emotion_variation / 100, 0.1), 1.0)
            
        except:
            return 0.6
    
    def _analyze_text_content(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ë‚´ìš© ë¶„ì„"""
        try:
            # 1. ì¤‘ë³µ ë‹¨ì–´ ì‚¬ìš© ë¶„ì„
            words = text.split()
            word_count = len(words)
            unique_words = len(set(words))
            redundancy_score = 1 - (unique_words / word_count) if word_count > 0 else 0
            
            # 2. ê¸ì •/ë¶€ì • ë‹¨ì–´ ë¶„ì„
            positive_words = ['ì¢‹ë‹¤', 'ì˜', 'ì„±ê³µ', 'ê¸ì •', 'íš¨ê³¼', 'ê°œì„ ', 'ë°œì „', 'ì„±ì¥']
            negative_words = ['ë‚˜ì˜ë‹¤', 'ì‹¤íŒ¨', 'ë¶€ì •', 'ë¬¸ì œ', 'ì–´ë ¤ì›€', 'ì‹¤íŒ¨', 'ì‹¤ìˆ˜']
            
            positive_count = sum(1 for word in words if word in positive_words)
            negative_count = sum(1 for word in words if word in negative_words)
            
            positive_word_ratio = positive_count / word_count if word_count > 0 else 0.5
            negative_word_ratio = negative_count / word_count if word_count > 0 else 0.1
            
            # 3. ì „ë¬¸ ìš©ì–´ ì‚¬ìš© ë¶„ì„
            technical_terms = ['ì•Œê³ ë¦¬ì¦˜', 'ë°ì´í„°ë² ì´ìŠ¤', 'API', 'í”„ë ˆì„ì›Œí¬', 'ì•„í‚¤í…ì²˜', 'ìµœì í™”', 'ì„±ëŠ¥']
            technical_term_count = sum(1 for word in words if word in technical_terms)
            
            # 4. ë¬¸ë²• ì˜¤ë¥˜ (ê°„ë‹¨í•œ ì¶”ì •)
            grammar_error_count = 0  # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¶„ì„ í•„ìš”
            
            # 5. ìš”ì•½ë ¥ (ë‹µë³€ ê¸¸ì´ ê¸°ë°˜)
            conciseness_score = 0.7 if 10 <= word_count <= 50 else 0.5
            
            # 6. ì°½ì˜ì„± (ë‹¤ì–‘í•œ ë‹¨ì–´ ì‚¬ìš©)
            creativity_score = unique_words / word_count if word_count > 0 else 0.6
            
            return {
                "redundancy_score": redundancy_score,
                "positive_word_ratio": positive_word_ratio,
                "negative_word_ratio": negative_word_ratio,
                "technical_term_count": technical_term_count,
                "grammar_error_count": grammar_error_count,
                "conciseness_score": conciseness_score,
                "creativity_score": creativity_score,
                "question_understanding_score": 0.8,  # ê¸°ë³¸ê°’
                "conversation_flow_score": 0.75,  # ê¸°ë³¸ê°’
                "total_silence_time": 1.0  # ê¸°ë³¸ê°’
            }
            
        except Exception as e:
            logging.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_default_text_analysis()
    
    def _get_default_audio_analysis(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼"""
        return {
            "speech_rate": 150.0,
            "volume_level": 0.75,
            "pronunciation_score": 0.85,
            "intonation_score": 0.6,
            "emotion_variation": 0.6,
            "background_noise_level": 0.1
        }
    
    def _get_default_video_analysis(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¹„ë””ì˜¤ ë¶„ì„ ê²°ê³¼"""
        return {
            "smile_frequency": 1.0,
            "eye_contact_ratio": 0.8,
            "hand_gesture": 0.5,
            "nod_count": 2,
            "posture_changes": 2,
            "eye_aversion_count": 1,
            "facial_expression_variation": 0.6
        }
    
    def _get_default_text_analysis(self) -> Dict[str, Any]:
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼"""
        return {
            "redundancy_score": 0.05,
            "positive_word_ratio": 0.6,
            "negative_word_ratio": 0.1,
            "technical_term_count": 5,
            "grammar_error_count": 1,
            "conciseness_score": 0.7,
            "creativity_score": 0.6,
            "question_understanding_score": 0.8,
            "conversation_flow_score": 0.75,
            "total_silence_time": 1.0
        }

def analyze_interview_video(video_path: str, output_path: str = None) -> Dict[str, Any]:
    """ë©´ì ‘ ì˜ìƒ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜"""
    pipeline = VideoAnalysisPipeline()
    analysis_result = pipeline.analyze_video(video_path)
    
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")
    
    return analysis_result

if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    video_path = "interview_video.mp4"
    result = analyze_interview_video(video_path, "analysis_result.json")
    print("ğŸ‰ ì˜ìƒ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“Š ë¶„ì„ í•­ëª© ìˆ˜: {len(result)}") 